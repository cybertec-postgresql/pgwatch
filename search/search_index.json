{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>pgwatch is a flexible PostgreSQL-specific monitoring solution, relying on Grafana dashboards for the UI part. It supports monitoring of almost all metrics for Postgres versions 9.0 to 13 out of the box and can be easily extended to include custom metrics. At the core of the solution is the metrics gathering daemon written in Go, with many options to configure the details and aggressiveness of monitoring, types of metrics storage and the display the metrics.</p>"},{"location":"index.html#quick-start-with-docker","title":"Quick start with Docker","text":"<p>For the fastest setup experience Docker images are provided via Docker Hub (if new to Docker start here). For custom setups see the Custom installations paragraph below or turn to the pre-built DEB / RPM / Tar packages on the Github Releases page.</p> <p>Launching the latest pgwatch Docker image with built-in Postgres metrics storage DB:</p> <pre><code># run the latest Docker image, exposing Grafana on port 3000 and the administrative web UI on 8080\ndocker run -d -p 3000:3000 -p 8080:8080 -e PW_TESTDB=true --name pw3 cybertec/pgwatch\n</code></pre> <p>After some minutes you could for example open the DB overview dashboard and start looking at metrics in Grafana. For defining your own dashboards or making changes you need to log in as admin (default user/password: <code>admin/pgwatchadmin</code>).</p> <p>If you don't want to add the <code>\"test\"</code> database (the pgwatch configuration DB holding connection strings to monitored DBs and metric definitions) to monitoring, remove the <code>PW_TESTDB</code> env variable.</p> <p>Also note that for long term production usage with Docker it's highly recommended to use separate volumes for each pgwatch component - see here for a better launch example.</p>"},{"location":"index.html#typical-pull-architecture","title":"Typical \"pull\" architecture","text":"<p>To get an idea how pgwatch is typically deployed a diagram of the standard Docker image fetching metrics from a set of Postgres databases configured via a configuration DB:</p> <p></p>"},{"location":"index.html#typical-push-architecture","title":"Typical \"push\" architecture","text":"<p>A better fit for very dynamic (Cloud) environments might be a more de-centralized \"push\" approach or just exposing the metrics over a port for remote scraping. In that case the only component required would be the pgwatch metrics collection daemon.</p> <p></p>"},{"location":"CODE_OF_CONDUCT.html","title":"Citizen Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT.html#1-purpose","title":"1. Purpose","text":"<p>A primary goal of pgwatch is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof).</p> <p>This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior.</p> <p>We invite all those who participate in pgwatch to help us create safe and positive experiences for everyone.</p>"},{"location":"CODE_OF_CONDUCT.html#2-open-sourceculturetech-citizenship","title":"2. Open [Source/Culture/Tech] Citizenship","text":"<p>A supplemental goal of this Code of Conduct is to increase open [source/culture/tech] citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community.</p> <p>Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society.</p> <p>If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know.</p>"},{"location":"CODE_OF_CONDUCT.html#3-expected-behavior","title":"3. Expected Behavior","text":"<p>The following behaviors are expected and requested of all community members:</p> <ul> <li>Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community.</li> <li>Exercise consideration and respect in your speech and actions.</li> <li>Attempt collaboration before conflict.</li> <li>Refrain from demeaning, discriminatory, or harassing behavior and speech.</li> <li>Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential.</li> <li>Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations.</li> </ul>"},{"location":"CODE_OF_CONDUCT.html#4-unacceptable-behavior","title":"4. Unacceptable Behavior","text":"<p>The following behaviors are considered harassment and are unacceptable within our community:</p> <ul> <li>Violence, threats of violence or violent language directed against another person.</li> <li>Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language.</li> <li>Posting or displaying sexually explicit or violent material.</li> <li>Posting or threatening to post other people's personally identifying information (\"doxing\").</li> <li>Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability.</li> <li>Inappropriate photography or recording.</li> <li>Inappropriate physical contact. You should have someone's consent before touching them.</li> <li>Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances.</li> <li>Deliberate intimidation, stalking or following (online or in person).</li> <li>Advocating for, or encouraging, any of the above behavior.</li> <li>Sustained disruption of community events, including talks and presentations.</li> </ul>"},{"location":"CODE_OF_CONDUCT.html#5-weapons-policy","title":"5. Weapons Policy","text":"<p>No weapons will be allowed at pgwatch events, community spaces, or in other spaces covered by the scope of this Code of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. Community members are further expected to comply with all state and local laws on this matter.</p>"},{"location":"CODE_OF_CONDUCT.html#6-consequences-of-unacceptable-behavior","title":"6. Consequences of Unacceptable Behavior","text":"<p>Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated.</p> <p>Anyone asked to stop unacceptable behavior is expected to comply immediately.</p> <p>If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event).</p>"},{"location":"CODE_OF_CONDUCT.html#7-reporting-guidelines","title":"7. Reporting Guidelines","text":"<p>If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. Pavlo Golub.</p> <p>Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress.</p>"},{"location":"CODE_OF_CONDUCT.html#8-addressing-grievances","title":"8. Addressing Grievances","text":"<p>If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify cybertec-postgresql with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. </p>"},{"location":"CODE_OF_CONDUCT.html#9-scope","title":"9. Scope","text":"<p>We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues--online and in-person--as well as in all one-on-one communications pertaining to community business.</p> <p>This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members.</p>"},{"location":"CODE_OF_CONDUCT.html#10-contact-info","title":"10. Contact info","text":"<p>Pavlo Golub Cybertec</p>"},{"location":"CODE_OF_CONDUCT.html#11-license-and-attribution","title":"11. License and attribution","text":"<p>The Citizen Code of Conduct is distributed by Stumptown Syndicate under a Creative Commons Attribution-ShareAlike license. </p> <p>Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy.</p> <p>Revision 2.3. Posted 6 March 2017.</p> <p>Revision 2.2. Posted 4 February 2016.</p> <p>Revision 2.1. Posted 23 June 2014.</p> <p>Revision 2.0, adopted by the Stumptown Syndicate board on 10 January 2013. Posted 17 March 2013.</p>"},{"location":"ENV_VARIABLES.html","title":"Available env. variables by components","text":"<p>Some variables influence multiple components. Command line parameters override env. variables (when doing custom deployments).</p>"},{"location":"ENV_VARIABLES.html#docker-image-specific","title":"Docker image specific","text":"<ul> <li>PW_TESTDB When set, the config DB itself will be added to monitoring as \"test\". Default: -</li> </ul>"},{"location":"ENV_VARIABLES.html#gatherer-daemon","title":"Gatherer daemon","text":"<p>See <code>pgwatch --help</code> output for details.</p>"},{"location":"ENV_VARIABLES.html#grafana","title":"Grafana","text":"<ul> <li>PW_GRAFANANOANONYMOUS Can be set to require login even for viewing dashboards. Default: -</li> <li>PW_GRAFANAUSER Administrative user. Default: admin</li> <li>PW_GRAFANAPASSWORD Administrative user password. Default: pgwatchadmin</li> <li>PW_GRAFANASSL Use SSL. Default: -</li> <li>PW_GRAFANA_BASEURL For linking to Grafana \"Query details\" dashboard from \"Stat_stmt. overview\". Default: http://0.0.0.0:3000</li> </ul>"},{"location":"LICENSE.html","title":"License","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2022, CYBERTEC PostgreSQL International GmbH All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"advanced_features.html","title":"Advanced features","text":"<p>Over the years the core functionality of fetching metrics from a set of plain Postgres DB-s has been extended in many ways to cover some common problem areas like server log monitoring and supporting monitoring of some other popular tools often used together with Postgres, like the PgBouncer connection pooler for example.</p>"},{"location":"advanced_features.html#patroni-support","title":"Patroni support","text":"<p>Patroni is a popular Postgres specific HA-cluster manager that makes node management simpler than ever, meaning that everything is dynamic though - cluster members can come and go, making monitoring in the standard way a bit tricky. But luckily Patroni cluster members information is stored in a DCS (Distributed Consensus Store), like etcd, so it can be fetched from there periodically.</p> <p>When 'patroni' is selected as a source type then the usual Postgres  host/port fields should be left empty (\"dbname\" can still filled if only a  specific single database is to be monitored) and instead \"Host config\" JSON field should be filled with DCS address, type and scope (cluster name) information. A sample config (for Config DB based setups) looks like:</p> <pre><code>    {\n      \"dcs_type\": \"etcd\",\n      \"dcs_endpoints\": [\"http://127.0.0.1:2379\"],\n      \"scope\": \"batman\",\n      \"namespace\": \"/service/\"\n    }\n</code></pre> <p>For YAML based setups an example can be found from the instances.yaml file.</p> <p>If Patroni is powered by etcd, then also username, password, ca_file, cert_file, key_file optional security parameters can be defined - other DCS systems are currently only supported without authentication.</p> <p>Also if you don't use the standby nodes actively for queries then it might make sense to decrease the volume of gathered metrics and to disable the monitoring of such nodes with the \"Master mode only?\" checkbox (when using the Web UI) or with only_if_master=true if using a YAML based setup.</p>"},{"location":"advanced_features.html#log-parsing","title":"Log parsing","text":"<p>As of v1.7.0 the metrics collector daemon, when running on a DB server (controlled best over a YAML config), has capabilities to parse the database server logs for errors. Out-of-the-box it will though only work when logs are written in CSVLOG format. For other formats user needs to specify a regex that parses out named groups of following fields: database_name, error_severity. See here for an example regex.</p> <p>Note that only the event counts are stored, no error texts, usernames or other infos! Errors are grouped by severity for the monitored DB and for the whole instance. The metric name to enable log parsing is \"server_log_event_counts\". Also note that for auto-detection of log destination / setting to work, the monitoring user needs superuser / pg_monitor privileges - if this is not possible then log settings need to be specified manually under \"Host config\" as seen for example here.</p> <p>Sample configuration if not using CSVLOG logging:</p> <p>On Postgres side (on the monitored DB) <pre><code>    # Debian / Ubuntu default log_line_prefix actually\n    log_line_prefix = '%m [%p] %q%u@%d '\n</code></pre> YAML config (recommended when \"pushing\" metrics from DB nodes to a central metrics DB) <pre><code>    ## logs_glob_path is only needed if the monitoring user is cannot auto-detect it (i.e. not a superuser / pg_monitor role)\n    # logs_glob_path:\n    logs_match_regex: '^(?P&lt;log_time&gt;.*) \\[(?P&lt;process_id&gt;\\d+)\\] (?P&lt;user_name&gt;.*)@(?P&lt;database_name&gt;.*?) (?P&lt;error_severity&gt;.*?): '\n</code></pre> For log parsing to work the metric server_log_event_counts needs to be enabled or a preset config including it used - like the \"full\" preset.</p>"},{"location":"advanced_features.html#pgbouncer-support","title":"PgBouncer support","text":"<p>pgwatch also supports collecting internal statistics from the PgBouncer connection pooler, via the built-in special \"pgbouncer\" database and the <code>SHOW STATS</code> command. To enable it choose the according DB Type, provide connection info to the pooler port and make sure the pgbouncer_stats metric or \"pgbouncer\" preset config is selected for the host. Note that for the \"DB Name\" field you should insert not \"pgbouncer\" (although this special DB provides all the statistics) but the real name of the pool you wish to monitor or leave it empty to track all pools. In latter case individual pools will be identified / separated via the \"database\" tag.</p> <p>There's also a built-in Grafana dashboard for PgBouncer data, looking like that:</p> <p></p>"},{"location":"advanced_features.html#pgpool-ii-support","title":"Pgpool-II support","text":"<p>Quite similar to PgBouncer, also Pgpool offers some statistics on pool performance and status, which might be of interest especially if using the load balancing features. To enable it choose the according DB Type, provide connection info to the pooler port and make sure the pgpool_stats metric / preset config is selected for the host.</p> <p>The built-in Grafana dashboard for Pgpool data looks something like that:</p> <p></p>"},{"location":"advanced_features.html#prometheus-scraping","title":"Prometheus scraping","text":"<p>pgwatch was originally designed with direct metrics storage in mind, but later also support for externally controlled Prometheus scraping was added. Note that currently though the storage modes are exclusive, i.e. when you enable the Promotheus endpoint (default port 9187) there will be no direct metrics storage.</p> <p>To enable the scraping endpoint set <code>--datastore=prometheus</code> and optionally also <code>--prometheus-port</code>, <code>--prometheus-namespace</code>, <code>--prometheus-listen-addr</code>. Additionally note that you still need to specify some metrics config as usually - only metrics with interval values bigger than zero will be populated on scraping.</p> <p>Currently a few built-in metrics that require some state to be stored between scrapes, e.g. the \"change_events\" metric, will currently be ignored. Also non-numeric data columns will be ignored! Tag columns will be preserved though as Prometheus \"labels\".</p>"},{"location":"advanced_features.html#cloud-providers-support","title":"Cloud providers support","text":"<p>Due to popularity of various managed PostgreSQL offerings there's also support for some managed options in sense of Preset Configs, that take into account the fact that on such platforms you get a limited user that doesn't have access to all metrics or some features have just been plain removed. Thus to reduce server log errors and save time on experimenting there are following presets available:</p> <ul> <li>aws - for standard AWS RDS managed PostgreSQL databases</li> <li>aurora - for AWS Aurora managed PostgreSQL service</li> <li>azure - for Azure Database for PostgreSQL managed databases</li> <li>gce - for Google Cloud SQL for PostgreSQL managed databases</li> </ul>"},{"location":"components.html","title":"Components","text":"<p>The main development idea around pgwatch was to do the minimal work needed and not to reinvent the wheel - meaning that pgwatch is mostly just about gluing together already some proven pieces of software for metrics storage and using Grafana for dashboarding. So here a listing of components that can be used to build up a monitoring setup around the pgwatch metrics collector. Note that most components are not mandatory and for tasks like metrics storage there are many components to choose from.</p>"},{"location":"components.html#the-metrics-gathering-daemon","title":"The metrics gathering daemon","text":"<p>The metrics collector, written in Go, is the only mandatory and most critical component of the whole solution. The main task of the pgwatch collector / daemon is pretty simple - reading the configuration and metric defintions, fetching the metrics from the configured databases using the configured connection info and finally storing the metrics to some other database, or just exposing them over a port for scraping in case of Prometheus mode.</p>"},{"location":"components.html#configuration","title":"Configuration","text":"<p>The configuration says which databases, how often and with which metrics (SQL-s queries) are to be gathered. There are 3 options to store the configuration:</p> <ul> <li>A PostgreSQL database holding a simple schema with 5 tables;</li> <li>File based approach - YAML config file.</li> </ul>"},{"location":"components.html#measurements-storage","title":"Measurements storage","text":"<p>Many options here so that one can for example go for maximum storage effectiveness or pick something where they already know the query language:</p>"},{"location":"components.html#postgresql","title":"PostgreSQL","text":"<p>PostgreSQL is a world's most advanced Open Source RDBMS.</p> <p>Postgres storage is based on the JSONB datatype so minimally version 9.4+ is required, but for bigger setups where partitioning is a must, v11+ is needed. Any already existing Postgres database will do the trick, see the Bootstrapping the Metrics DB section for2 details.</p>"},{"location":"components.html#timescaledb","title":"TimescaleDB","text":"<p>TimescaleDB is a time-series extension for PostgreSQL.</p> <p>Although technically a plain extension it's often mentioned as a separate database system as it brings custom data compression to the table, enabling huge disk savings over standard Postgres. Note that pgwatch does not use Timescale's built-in retention management but a custom version.</p>"},{"location":"components.html#prometheus","title":"Prometheus","text":"<p>Prometheus is a time series database and monitoring system.</p> <p>Though Prometheus is not a traditional database system, it's a good choice for monitoring Cloud-like environments as the monitoring targets don't need to know too much about how actual monitoring will be carried out later and also Prometheus has a nice fault-tolerant alerting system for enterprise needs. By default Prometheus is not set up for long term metrics storage!</p>"},{"location":"components.html#json-files","title":"JSON files","text":"<p>Plain text files for testing / special use cases.</p>"},{"location":"components.html#the-web-ui","title":"The Web UI","text":"<p>The second homebrewn component of the pgwatch solution is an optional and relatively simple Web UI for administering details of the monitoring configuration like which databases should be monitored, with which metrics and intervals. Besides that there are some basic overview tables to analyze the gathered data and also possibilities to delete unneeded metric data (when removing a test host for example).</p> <p>Note</p> <p>Note that the Web UI can only be used if storing the configuration in the database (Postgres).</p>"},{"location":"components.html#metrics-representation","title":"Metrics representation","text":"<p>Standard pgwatch setup uses Grafana for analyzing the gathered metrics data in a visual, point-and-click way. For that a rich set of predefined dashboards for Postgres is provided, that should cover the needs of most users - advanced users would mostly always want to customize some aspects though, so it's not meant as a one-size-fits-all solution. Also as metrics are stored in a DB, they can be visualized or processed in any other way.</p>"},{"location":"components.html#component-diagram","title":"Component diagram","text":"<p>Component diagram of a typical setup:</p> <p></p>"},{"location":"components.html#component-reuse","title":"Component reuse","text":"<p>All components are loosely coupled, thus for non-pgwatch components (pgwatch components are only the metrics collector and the optional Web UI) you can decide to make use of an already existing installation of Postgres, Grafana or Prometheus and run additionally just the pgwatch collector.</p>"},{"location":"components.html#to-use-an-existing-postgres-db-for-storing-the-monitoring-config","title":"To use an existing Postgres DB for storing the monitoring config","text":"<p>Create a new pgwatch DB, preferrably also an accroding role who owns it. Then roll out the schema (pgwatch/sql/config_store/config_store.sql) and set the following parameters when running the image: <code>PW_PGHOST</code>, <code>PW_PGPORT</code>, <code>PW_PGDATABASE</code>, <code>PW_PGUSER</code>, <code>PW_PGPASSWORD</code>, <code>PW_PGSSL</code> (optional).</p>"},{"location":"components.html#to-use-an-existing-grafana-installation","title":"To use an existing Grafana installation","text":"<p>Load the pgwatch dashboards from grafana_dashboard folder if needed (one can totally define their own) and set the following paramater: <code>PW_GRAFANA_BASEURL</code>. This parameter only provides correct links to Grafana dashboards from the Web UI. Grafana is the most loosely coupled component for pgwatch and basically doesn't have to be used at all. One can make use of the gathered metrics directly over the Postgres (or Graphite) API-s.</p>"},{"location":"components.html#to-use-an-existing-postgres-db-for-storing-metrics","title":"To use an existing Postgres DB for storing metrics","text":"<ol> <li>Roll out the metrics storage schema according to instructions     from here.</li> <li> <p>Following parameters need to be set for the gatherer:</p> <ul> <li><code>--datastore=postgres</code> or <code>PW_DATASTORE=postgres</code></li> <li><code>--pg-metric-store-conn-str=\"postgresql://user:pwd@host:port/db\"</code>     or <code>PW_PG_METRIC_STORE_CONN_STR=\"...\"</code></li> <li>optionally also adjust the <code>--pg-retention-days</code> parameter. By     default 14 days for Postgres are kept</li> </ul> </li> <li> <p>If using the Web UI also set the datastore parameters     <code>--datastore</code> and <code>--pg-metric-store-conn-str</code> if wanting to     have an option to be able to clean up data also via the UI in a     more targeted way.</p> </li> </ol> <p>When using Postgres metrics storage, the schema rollout script activates \"asynchronous commiting\" feature for the pgwatch role in the metrics storage DB by default! If this is not wanted (no metrics can be lost in case of a crash), then re-enstate normal (synchronous) commiting with below query and restart the pgwatch agent: <pre><code>ALTER ROLE pgwatch IN DATABASE $MY_METRICS_DB SET synchronous_commit TO on;\n</code></pre></p>"},{"location":"contributing.html","title":"Contributing to PGWatch","text":"<p>Thank you for considering contributing to PGWatch! Here are some guidelines to help you get started.</p>"},{"location":"contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct.</p>"},{"location":"contributing.html#communication-channels","title":"Communication Channels","text":"<p>The main communication channel for the project is the  GitHub repository.  Feel free to open issues and participate in discussions there.</p>"},{"location":"contributing.html#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<p>To set up the development environment, please refer to the instructions in the  README.md file. We use Docker Compose to simplify the setup process.</p>"},{"location":"contributing.html#how-to-report-bugs","title":"How to Report Bugs","text":"<p>If you encounter any bugs, please report them by opening an issue in the  GitHub repository issues section.  Provide as much detail as possible to help us understand and resolve the issue.</p>"},{"location":"contributing.html#how-to-request-features","title":"How to Request Features","text":"<p>If you have a feature request, please start a discussion in the  GitHub repository discussions section.  We value your feedback and ideas!</p>"},{"location":"contributing.html#submitting-changes","title":"Submitting Changes","text":"<p>Before submitting any changes, please discuss them in the  GitHub repository discussions section.  This helps ensure that your contribution aligns with the project goals and prevents duplicate efforts.</p> <p>When you are ready to submit your changes, create a pull request. Make sure your pull request:</p> <ul> <li>Follows the Go Style Guide</li> <li>Includes tests for any new functionality or bug fixes</li> </ul>"},{"location":"contributing.html#coding-standards","title":"Coding Standards","text":"<p>We follow the Go Style Guide.  Please ensure your code adheres to these guidelines.</p>"},{"location":"contributing.html#testing","title":"Testing","text":"<p>We require tests for all changes. Please use the standard Go testing facilities.  Ensure that all tests pass before submitting your pull request.</p>"},{"location":"contributing.html#documentation","title":"Documentation","text":"<p>Documentation for the project resides in the same repository. If you make changes  that require documentation updates, please include those changes in your pull request.</p>"},{"location":"contributing.html#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>We do not require contributors to sign a Contributor License Agreement (CLA).  By submitting a pull request, you agree that your contributions are submitted  under the same license as the project.</p> <p>We appreciate your contributions and efforts to improve PGWatch. If you have any questions,  feel free to reach out through the GitHub repository.</p> <p>Thank you!</p>"},{"location":"custom_installation.html","title":"Custom installation","text":"<p>As described in the Components  chapter, there a couple of ways how to set up up pgwatch. Two most common ways though are the central Config DB based \"pull\" approach and the YAML file based \"push\" approach, plus Grafana to visualize the gathered metrics.</p>"},{"location":"custom_installation.html#config-db-based-setup","title":"Config DB based setup","text":""},{"location":"custom_installation.html#overview-of-installation-steps","title":"Overview of installation steps","text":"<ol> <li>Install Postgres or use any available existing instance - v9.4+     required for the config DB and v11+ for the metrics DB.</li> <li>Bootstrap the Config DB.</li> <li>Bootstrap the metrics storage DB (PostgreSQL here).</li> <li>Install pgwatch - either from pre-built packages or by compiling     the Go code.</li> <li>Prepare the \"to-be-monitored\" databases for monitoring by creating     a dedicated login role name as a minimum.</li> <li>Optional step - install the administrative Web UI + Python &amp; library     dependencies.</li> <li>Add some databases to the monitoring configuration via the Web UI or     directly in the Config DB.</li> <li>Start the pgwatch metrics collection agent and monitor the logs for     any problems.</li> <li>Install and configure Grafana and import the pgwatch sample     dashboards to start analyzing the metrics.</li> <li>Make sure that there are auto-start SystemD services for all     components in place and optionally set up also backups.</li> </ol>"},{"location":"custom_installation.html#detailed-steps-for-the-config-db-based-pull-approach-with-postgres-metrics-storage","title":"Detailed steps for the Config DB based \"pull\" approach with Postgres metrics storage","text":"<p>Below are sample steps to do a custom install from scratch using Postgres for the pgwatch configuration DB, metrics DB and Grafana config DB.</p> <p>All examples here assume Ubuntu as OS - but it's basically the same for RedHat family of operations systems also, minus package installation syntax differences.</p> <ol> <li> <p>Install Postgres</p> <p>Follow the standard Postgres install procedure basically. Use the latest major version available, but minimally v11+ is recommended for the metrics DB due to recent partitioning speedup improvements and also older versions were missing some default JSONB casts so that a few built-in Grafana dashboards need adjusting otherwise.</p> <p>To get the latest Postgres versions, official Postgres PGDG repos are to be preferred over default disto repos. Follow the instructions from:</p> <ul> <li>https://wiki.postgresql.org/wiki/Apt - for Debian / Ubuntu     based systems</li> <li>https://www.postgresql.org/download/linux/redhat/ - for CentOS     / RedHat based systems</li> </ul> </li> <li> <p>Install pgwatch - either from pre-built packages or by     compiling the Go code</p> <ul> <li> <p>Using pre-built packages</p> <p>The pre-built DEB / RPM / Tar packages are available on the Github releases page.</p> <pre><code># find out the latest package link and replace below, using v1.8.0 here\nwget https://github.com/cybertec-postgresql/pgwatch/releases/download/v1.8.0/pgwatch_v1.8.0-SNAPSHOT-064fdaf_linux_64-bit.deb\nsudo dpkg -i pgwatch_v1.8.0-SNAPSHOT-064fdaf_linux_64-bit.deb\n</code></pre> </li> <li> <p>Compiling the Go code yourself</p> <p>This method of course is not needed unless dealing with maximum security environments or some slight code changes are required.</p> <ol> <li> <p>Install Go by following the official     instructions</p> </li> <li> <p>Get the pgwatch project's code and compile the gatherer     daemon</p> <pre><code>git clone https://github.com/cybertec-postgresql/pgwatch.git\ncd pgwatch/internal/webui\nyarn install --network-timeout 100000 &amp;&amp; yarn build\ncd ..\ngo build\n</code></pre> <p>After fetching all the Go library dependencies (can take minutes) an executable named \"pgwatch\" should be generated. Additionally it's a good idea to copy it to <code>/usr/bin/pgwatch</code>.</p> </li> </ol> </li> <li> <p>Configure a SystemD auto-start service (optional)</p> <p>Sample startup scripts can be found at /etc/pgwatch/startup-scripts/pgwatch.service or online here. Note that they are OS agnostic and might need some light adjustment of paths, etc - so always test them out.</p> </li> </ul> </li> <li> <p>Boostrap the config DB</p> <ol> <li> <p>Create a user to \"own\" the <code>pgwatch</code> schema</p> <p>Typically called <code>pgwatch</code> but can be anything really, if the schema creation file is adjusted accordingly.</p> <pre><code>psql -c \"create user pgwatch password 'xyz'\"\npsql -c \"create database pgwatch owner pgwatch\"\n</code></pre> </li> <li> <p>Roll out the pgwatch config schema</p> <p>The schema will most importantly hold connection strings of DB-s to be monitored and the metric definitions.</p> <pre><code># FYI - one could get the below schema files also directly from Github\n# if re-using some existing remote Postgres instance where pgwatch was not installed\npsql -f /etc/pgwatch/sql/config_store/config_store.sql pgwatch\npsql -f /etc/pgwatch/sql/config_store/metric_definitions.sql pgwatch\n</code></pre> </li> </ol> </li> <li> <p>Bootstrap the measurements storage DB</p> <ol> <li> <p>Create a dedicated database for storing metrics and a user to     \"own\" the metrics schema</p> <p>Here again default scripts expect a role named <code>pgwatch</code> but can be anything if to adjust the scripts.</p> <pre><code>psql -c \"create database pgwatch_metrics owner pgwatch\"\n</code></pre> </li> <li> <p>Roll out the pgwatch metrics storage schema</p> <p>This is a place to pause and first think how many databases will be monitored, i.e. how much data generated, and based on that one should choose an according metrics storage schema. There are a couple of different options available that are described here in detail, but the gist of it is that you don't want too complex partitioning schemes if you don't have zounds of data and don't need the fastest queries. For a smaller amount of monitored DBs (a couple dozen to a hundred) the default \"metric-time\" is a good choice. For hundreds of databases, aggressive intervals, or long term storage usage of the TimescaleDB extension is recommended.</p> <pre><code>cd /etc/pgwatch/sql/metric_store\npsql -f roll_out_metric_time.psql pgwatch_metrics\n</code></pre> <p>Note</p> <p>Default retention for Postgres storage is 2 weeks! To change, use the <code>--pg-retention-days / PW_PG_RETENTION_DAYS</code> gatherer parameter.</p> </li> </ol> </li> <li> <p>Prepare the \"to-be-monitored\" databases for metrics collection</p> <p>As a minimum we need a plain unprivileged login user. Better though is to grant the user also the <code>pg_monitor</code> system role, available on v10+. Superuser privileges should be normally avoided for obvious reasons of course, but for initial testing in safe environments it can make the initial preparation (automatic helper rollouts) a bit easier still, given superuser privileges are later stripped.</p> <p>To get most out of your metrics some <code>SECURITY DEFINER</code> wrappers functions called \"helpers\" are recommended on the DB-s under monitoring. See the detailed chapter on the \"preparation\" topic here for more details.</p> </li> <li> <p>Configure DB-s and metrics / intervals to be monitored</p> <ul> <li>From the Web UI \"/dbs\" page</li> <li>Via direct inserts into the Config DB <code>pgwatch.monitored_db</code> table</li> </ul> </li> <li> <p>Start the pgwatch metrics collection agent</p> <ol> <li> <p>The gatherer has quite some parameters (use the <code>--help</code> flag     to show them all), but simplest form would be:</p> <pre><code>pgwatch-daemon \\\n  --host=localhost --user=pgwatch --dbname=pgwatch \\\n  --datastore=postgres --pg-metric-store-conn-str=postgresql://pgwatch@localhost:5432/pgwatch_metrics \\\n  --verbose=info\n</code></pre> <p>Default connections params expect a trusted localhost Config DB setup so mostly the 2nd line is not needed actually.</p> <p>Or via SystemD if set up in previous steps</p> <pre><code>useradd -m -s /bin/bash pgwatch # default SystemD templates run under the pgwatch user\nsudo systemctl start pgwatch\nsudo systemctl status pgwatch\n</code></pre> <p>After initial verification that all works it's usually good idea to set verbosity back to default by removing the verbose flag.</p> <p>Another tip to configure connection strings inside SystemD service files is to use the \"systemd-escape\" utility to escape special characters like spaces etc if using the LibPQ connect string syntax rather than JDBC syntax.</p> </li> <li> <p>Monitor the console or log output for any problems</p> <p>If you see metrics trickling into the \"pgwatch_metrics\" database (metric names are mapped to table names and tables are auto-created), then congratulations - the deployment is working! When using some more aggressive preset metrics config then there are usually still some errors though, due to the fact that some more extensions or privileges are missing on the monitored database side. See the according chapter here.</p> </li> </ol> <p>Info</p> <p>When you're compiling your own gatherer then the executable file will be named just <code>pgwatch</code> instead of <code>pgwatch-daemon</code> to avoid mixups.</p> </li> <li> <p>Install Grafana</p> <ol> <li> <p>Create a Postgres database to hold Grafana internal config, like     dashboards etc</p> <p>Theoretically it's not absolutely required to use Postgres for storing Grafana internal settings / dashboards, but doing so has 2 advantages - you can easily roll out all pgwatch built-in dashboards and one can also do remote backups of the Grafana configuration easily.</p> <pre><code>psql -c \"create user pgwatch_grafana password 'xyz'\"\npsql -c \"create database pgwatch_grafana owner pgwatch_grafana\"\n</code></pre> </li> <li> <p>Follow the instructions from     https://grafana.com/docs/grafana/latest/installation/debian/,     basically something like:</p> <pre><code>wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\necho \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\nsudo apt-get update &amp;&amp; sudo apt-get install grafana\n\n# review / change config settings and security, etc\nsudo vi /etc/grafana/grafana.ini\n\n# start and enable auto-start on boot\nsudo systemctl daemon-reload\nsudo systemctl start grafana-server\nsudo systemctl status grafana-server\n</code></pre> <p>Default Grafana port: 3000</p> </li> <li> <p>Configure Grafana config to use our <code>pgwatch_grafana</code> DB</p> <p>Place something like below in the <code>[database]</code> section of <code>/etc/grafana/grafana.ini</code></p> <pre><code>[database]\ntype = postgres\nhost = my-postgres-db:5432\nname = pgwatch_grafana\nuser = pgwatch_grafana\npassword = xyz\n</code></pre> <p>Taking a look at <code>[server], [security]</code> and <code>[auth*]</code> sections is also recommended.</p> </li> <li> <p>Set up the <code>pgwatch</code> metrics database as the default datasource</p> <p>We need to tell Grafana where our metrics data is located. Add a datasource via the Grafana UI (Admin -&gt; Data sources) or adjust and execute the \"pgwatch/bootstrap/grafana_datasource.sql\" script on the <code>pgwatch_grafana</code> DB.</p> </li> <li> <p>Add pgwatch predefined dashboards to Grafana</p> <p>This could be done by importing the pgwatch dashboard definition JSON-s manually, one by one, from the \"grafana\" folder (\"Import Dashboard\" from the Grafana top menu) or via as small helper script located at /etc/pgwatch/grafana-dashboards/import_all.sh. The script needs some adjustment for metrics storage type, connect data and file paths.</p> </li> <li> <p>Optionally install also Grafana plugins</p> <p>Currently one pre-configured dashboard (Biggest relations treemap) use an extra plugin - if planning to that dash, then run the following:</p> <pre><code>grafana-cli plugins install savantly-heatmap-panel\n</code></pre> </li> <li> <p>Start discovering the preset dashbaords</p> <p>If the previous step of launching pgwatch daemon succeeded and it was more than some minutes ago, one should already see some graphs on dashboards like \"DB overview\" or \"DB overview Unprivileged / Developer mode\" for example.</p> </li> </ol> </li> </ol>"},{"location":"custom_installation.html#yaml-based-setup","title":"YAML based setup","text":"<p>From v1.4 one can also deploy the pgwatch gatherer daemons more easily in a de-centralized way, by specifying monitoring configuration via YAML files. In that case there is no need for a central Postgres \"config DB\".</p> <p>YAML installation steps</p> <ol> <li>Install pgwatch - either from pre-built packages or by compiling     the Go code.</li> <li>Specify hosts you want to monitor and with which metrics /     aggressivness in a YAML file or files, following the example config     located at /etc/pgwatch/config/instances.yaml or online     here.     Note that you can also use env. variables inside the YAML templates!</li> <li>Bootstrap the metrics storage DB (not needed it using Prometheus     mode).</li> <li>Prepare the \"to-be-monitored\" databases for monitoring by creating     a dedicated login role name as a     minimum.</li> <li>Run the pgatch2 gatherer specifying the YAML config file (or     folder), and also the folder where metric definitions are located.     Default location: /etc/pgwatch/metrics.</li> <li>Install and configure Grafana and import the pgwatch sample     dashboards to start analyzing the metrics. See above for     instructions.</li> <li>Make sure that there are auto-start SystemD services for all     components in place and optionally set up also backups.</li> </ol> <p>Relevant gatherer parameters / env. vars: <code>--config / PW_CONFIG</code> and <code>--metrics-folder / PW_METRICS_FOLDER</code>.</p> <p>For details on individual steps like installing pgwatch see the above paragraph.</p>"},{"location":"dashboarding_alerting.html","title":"Grafana intro","text":"<p>To display the gathered and stored metrics the pgwatch project has decided to rely heavily on the popular Grafana dashboarding solution. This means only though that it's installed in the default Docker images and there's a set of predefined dashboards available to cover most of the metrics gathered via the Preset Configs.</p> <p>This does not mean though that Grafana is in any way tightly coupled with project's other components - quite the opposite actually, one can use any other means / tools to use the metrics data gathered by the pgwatch daemon.</p> <p>Currently there are around 30 preset dashboards available for PostgreSQL data sources. Due to that nowadays, if metric gathering volumes are not a problem, we recommend using Postgres storage for most users.</p> <p>Note though that most users will probably want to always adjust the built-in dashboards slightly (colors, roundings, etc) , so that they should be taken only as examples to quickly get started. Also note that in case of changes it's not recommended to change the built-in ones, but use the Save as features - this will allow later to easily update all the dashboards en masse per script, without losing any custom user changes.</p> <p>Links:</p> <p>Built-in dashboards for PostgreSQL (TimescaleDB) storage</p> <p>Screenshots of pgwatch default dashboards</p> <p>The online Demo site</p>"},{"location":"dashboarding_alerting.html#alerting","title":"Alerting","text":"<p>Alerting is very conveniently also supported by Grafana in a simple point-and-click style - see here for the official documentation. In general all more popular notification services are supported and it's pretty much the easiest way to quickly start with PostgreSQL alerting on a smaller scale. For enterprise usage with hundreds of instances it's might get too \"clicky\" though and there are also some limitations - currently you can set alerts only on Graph panels and there must be no variables used in the query so you cannot use most of the pre-created pgwatch graphs, but need to create your own.</p> <p>Nevertheless, alerting via Grafana is s a good option for lighter use cases and there's also a preset dashboard template named \"Alert Template\" from the pgwatch project to give you some ideas on what to alert on.</p> <p>Note though that alerting is always a bit of a complex topic - it requires good understanding of PostgreSQL operational metrics and also business criticality background infos, so we don't want to be too opinionated here and it's up to the users to implement.</p>"},{"location":"docker_installation.html","title":"Installing using Docker","text":""},{"location":"docker_installation.html#simple-setup-steps","title":"Simple setup steps","text":"<p>The simplest real-life pgwatch setup should look something like that:</p> <ol> <li> <p>Decide which metrics storage engine you want to use -     cybertec/pgwatch uses PostgreSQL. For Prometheus mode (exposing a     port for remote scraping) one should use the slimmer     cybertec/pgwatch-daemon image which doesn't have any built in     databases.</p> </li> <li> <p>Find the latest pgwatch release version by going to the project's     Github Releases page or use the public API with something like     that:</p> <pre><code>curl -so- https://api.github.com/repos/cybertec-postgresql/pgwatch/releases/latest | jq .tag_name | grep -oE '[0-9\\.]+'\n</code></pre> </li> <li> <p>Pull the image:</p> <pre><code>docker pull cybertec/pgwatch:X.Y.Z\n</code></pre> </li> <li> <p>Run the Docker image, exposing minimally the Grafana port served on     port 3000 internally. In a relatively secure environment you'd     usually also include the administrative web UI served on port 8080:</p> <pre><code>docker run -d --restart=unless-stopped -p 3000:3000 -p 8080:8080 \\\n--name pw3 cybertec/pgwatch:X.Y.Z\n</code></pre> <p>Note that we're setting the container to be automatically restarted in case of a reboot/crash - which is highly recommended if not using some container management framework to run pgwatch.</p> </li> </ol>"},{"location":"docker_installation.html#more-future-proof-setup-steps","title":"More future proof setup steps","text":"<p>Although the above simple setup example will do for more temporal setups / troubleshooting sessions, for permanent setups it's highly recommended to create separate volumes for all software components in the container, so that it would be easier to update to newer pgwatch Docker images and pull file system based backups and also it might be a good idea to expose all internal ports at least on localhost for possible troubleshooting and making possible to use native backup tools more conveniently for Postgres.</p> <p>Note that for maximum flexibility, security and update simplicity it's best to do a custom setup though - see the next chapter for that.</p> <p>So in short, for plain Docker setups would be best to do something like:</p> <pre><code># let's create volumes for Postgres, Grafana and pgwatch marker files / SSL certificates\nfor v in pg  grafana pw3 ; do docker volume create $v ; done\n\n# launch pgwatch with fully exposed Grafana and Health-check ports\n# and local Postgres and subnet level Web UI ports\ndocker run -d --restart=unless-stopped --name pw3 \\\n    -p 3000:3000 -p 8081:8081 -p 127.0.0.1:5432:5432 -p 192.168.1.XYZ:8080:8080 \\\n    -v pg:/var/lib/postgresql -v grafana:/var/lib/grafana -v pw3:/pgwatch/persistent-config \\\n    cybertec/pgwatch:X.Y.Z\n</code></pre> <p>Note that in non-trusted environments it's a good idea to specify more sensitive ports together with some explicit network interfaces for additional security - by default Docker listens on all network devices!</p> <p>Also note that one can configure many aspects of the software components running inside the container via ENV - for a complete list of all supported Docker environment variables see the ENV_VARIABLES.md file.</p>"},{"location":"docker_installation.html#available-docker-images","title":"Available Docker images","text":"<p>Following images are regularly pushed to Docked Hub:</p> <p>cybertec/pgwatch-demo</p> <p>The original pgwatch \u201cbatteries-included\u201d image with PostgreSQL measurements storage. Just insert connect infos to your database via the admin Web UI (or directly into the Config DB) and then turn to the pre-defined Grafana dashboards to analyze DB health and performance.</p> <p>cybertec/pgwatch</p> <p>A light-weight image containing only the metrics collection daemon / agent, that can be integrated into the monitoring setup over configuration specified either via ENV, mounted YAML files or a PostgreSQL Config DB. See the Component reuse chapter for wiring details.</p>"},{"location":"docker_installation.html#building-custom-docker-images","title":"Building custom Docker images","text":"<p>For custom tweaks, more security, specific component versions, etc one could easily build the images themselves, just a Docker installation is needed.</p>"},{"location":"docker_installation.html#interacting-with-the-docker-container","title":"Interacting with the Docker container","text":"<ul> <li> <p>If to launch with the <code>PW_TESTDB=1</code> env. parameter then the     pgwatch configuration database running inside Docker is added to     the monitoring, so that you should immediately see some metrics at     least on the Health-check dashboard.</p> </li> <li> <p>To add new databases / instances to monitoring open the     administration Web interface on port 8080 (or some other port, if     re-mapped at launch) and go to the /dbs page. Note that the Web UI     is an optional component, and one can managed monitoring entries     directly in the Postgres Config DB via <code>INSERT</code>-s / <code>UPDATE</code>-s into     <code>\"pgwatch.monitored_db\"</code> table. Default user/password are again     <code>pgwatch/pgwatchadmin</code>, database name - <code>pgwatch</code>. In both     cases note that it can take up to 2min (default main loop time,     changeable via <code>PW_SERVERS_REFRESH_LOOP_SECONDS</code>) before you see     any metrics for newly inserted databases.</p> </li> <li> <p>One can edit existing or create new Grafana dashboards, change     Grafana global settings, create users, alerts, etc after logging in     as <code>pgwatch/pgwatchadmin</code> (by default, changeable at launch     time).</p> </li> <li> <p>Metrics and their intervals that are to be gathered can be     customized for every database separately via a custom JSON config     field or more conveniently by using Preset Configs, like     \"minimal\", \"basic\" or \"exhaustive\" (<code>monitored_db.preset_config</code>     table), where the name should already hint at the amount of metrics     gathered. For privileged users the \"exhaustive\" preset is a good     starting point, and \"unprivileged\" for simple developer accounts.</p> </li> <li> <p>To add a new metrics yourself (which are simple SQL queries     returning any values and a timestamp) head to     http://127.0.0.1:8080/metrics. The queries should always include a     <code>\"epoch_ns\"</code> column and <code>\"tag\\_\"</code> prefix can be used for columns     that should be quickly searchable/groupable, and thus will be     indexed with the PostgreSQL metric stores. See to the bottom of the     \"metrics\" page for more explanations or the documentation chapter     on metrics.</p> </li> <li> <p>For a quickstart on dashboarding, a list of available metrics     together with some instructions are presented on the     \"Documentation\" dashboard.</p> </li> <li> <p>Some built-in metrics like <code>\"cpu_load\"</code> and others, that gather     privileged or OS statistics, require installing helper functions     (looking like     that,     so it might be normal to see some blank panels or fetching errors in     the logs. On how to prepare databases for monitoring see the     Monitoring preparations chapter.</p> </li> <li> <p>For effective graphing you want to familiarize yourself with the     query language of the database system that was selected for metrics     storage. Some tips to get going:</p> <ul> <li>For PostgreSQL/TimescaleDB - some knowledge of Window     functions     is a must if looking at longer time periods of data as the     statistics could have been reset in the mean time by user     request or the server might have crashed, so that simple     <code>max() - min()</code> aggregates on cumulative counters (most data     provided by Postgres is cumulative) would lie.</li> </ul> </li> <li> <p>For possible troubleshooting needs, logs of the components running     inside Docker are by default (if not disabled on container launch)     visible under:     http://127.0.0.1:8080/logs/%5Bpgwatch%7Cpostgres%7Cwebui%7Cgrafana.     It's of course also possible to log into the container and look at     log files directly - they're situated under     <code>/var/logs/supervisor/</code>.</p> <p>FYI - <code>docker logs ...</code> command is not really useful after a successful container startup in pgwatch case.</p> </li> </ul>"},{"location":"docker_installation.html#ports-used","title":"Ports used","text":"<ul> <li>5432 - Postgres configuration or metrics storage DB (when using the     cybertec/pgwatch image)</li> <li>8080 - Management Web UI (monitored hosts, metrics, metrics     configurations)</li> <li>8081 - Gatherer healthcheck / statistics on number of gathered     metrics (JSON).</li> <li>3000 - Grafana dashboarding</li> </ul>"},{"location":"docker_installation.html#docker-compose","title":"Docker Compose","text":"<p>As mentioned in the Components chapter, remember that the pre-built Docker images are just one example how your monitoring setup around the pgwatch metrics collector could be organized. For another example how various components (as Docker images here) can work together, see a Docker Compose example with loosely coupled components here.</p>"},{"location":"features.html","title":"List of main features","text":"<ul> <li>Non-invasive setup on PostgreSQL side - no extensions nor superuser     rights are required for the base functionality so that even     unprivileged users like developers can get a good overview of     database activities without any hassle</li> <li>Lots of preset metric configurations covering all performance     critical PostgreSQL internal Statistics Collector data</li> <li>Intuitive metrics presentation using a set of predefined dashboards     for the very popular Grafana dashboarding engine with optional     alerting support</li> <li>Easy extensibility of metrics which are defined in pure SQL, thus     they could also be from the business domain</li> <li>Many metric data storage options - PostgreSQL, PostgreSQL with the     compression enabled TimescaleDB extension, or Prometheus scraping</li> <li>Multiple deployment options - PostgreSQL configuration DB, YAML or     ENV configuration, supporting both \"push\" and \"pull\" models</li> <li>Possible to monitoring all, single or a subset (list or regex) of     databases of a PostgreSQL instance</li> <li>Global or per DB configuration of metrics and metric fetching     intervals and optionally also times / days</li> <li>Kubernetes/OpenShift ready with sample templates and a Helm chart</li> <li>PgBouncer, Pgpool2, AWS RDS and Patroni support with automatic     member discovery</li> <li>Internal health-check API (port 8081 by default) to monitor metrics     gathering status / progress remotely</li> <li>Built-in security with SSL connections support for all components     and passwords encryption for connect strings</li> <li>Very low resource requirements for the collector even when     monitoring hundreds of instances</li> <li>Capabilities to go beyond PostgreSQL metrics gathering with built-in     log parsing for error detection and OS level metrics collection via     PL/Python \"helper\" stored procedures</li> <li>A Ping mode to test connectivity to all databases under monitoring</li> </ul>"},{"location":"gallery.html","title":"Gallery","text":""},{"location":"godoc.html","title":"Documentation","text":""},{"location":"installation_options.html","title":"Installation options","text":"<p>Besides freedom of choosing from a set of metric storage options one can also choose how they're going to retrieve metrics from databases - in a \"pull\" or \"push\" way and how is the monitoring configuration (connect strings, metric sets and intervals) going to be stored.</p>"},{"location":"installation_options.html#config-db-based-operation","title":"Config DB based operation","text":"<p>This is the original central pull mode depicted on the architecture diagram.  It requires a small schema to be rolled out on any Postgres database accessible to the metrics gathering daemon, which will hold the connect strings, metric definition SQL-s and preset configurations and some other more minor attributes. For rollout details see the custom installation chapter.</p> <p>The default Docker images use this approach.</p>"},{"location":"installation_options.html#file-based-operation","title":"File based operation","text":"<p>From v1.4.0 one can deploy the gatherer daemon(s) decentrally with hosts to be monitored defined in simple YAML files. In that case there is no need for the central Postgres \"config DB\". See the sample instances.yaml config file for an example. Note that in this mode you also need to point out the path to metric definition SQL files when starting the gatherer. Also note that the configuration system also supports multiple YAML files in a folder so that you could easily programmatically manage things via Ansible for example and you can also use Env. vars in sideYAML files.</p> <p>Relevant Gatherer env. vars / flags: <code>--config, --metrics-folder</code> or <code>PW_CONFIG / PW_METRICS_FOLDER</code>.</p>"},{"location":"installation_options.html#prometheus-mode","title":"Prometheus mode","text":"<p>In v1.6.0 was added support for Prometheus - being one of the most popular modern metrics gathering / alerting solutions. When the <code>--datastore / PW_DATASTORE</code> parameter is set to prometheus then the pgwatch metrics collector doesn't do any normal interval-based fetching but listens on port 9187 (changeable) for scrape requests configured and performed on Prometheus side. Returned metrics belong to the \"pgwatch\" namespace (a prefix basically) which is changeable via the <code>--prometheus-namespace</code> flag if needed.</p> <p>Also important to note - in this mode the pgwatch agent should not be run centrally but on all individual DB hosts. While technically possible though to run centrally, it would counter the core idea of Prometheus and would make scrapes also longer and risk getting timeouts as all DBs are scanned sequentially for metrics.</p> <p>FYI -- the functionality has some overlap with the existing postgres_exporter project, but also provides more flexibility in metrics configuration and all config changes are applied \"online\".</p> <p>Also note that Prometheus can only store numerical metric data values - so metrics output for example PostgreSQL storage and Prometheus are not 100% compatile. Due to that there's also a separate \"preset config\" named \"prometheus\".</p>"},{"location":"kubernetes.html","title":"Kubernetes","text":"<p>A basic Helm chart is available for installing pgwatch to a Kubernetes cluster. The corresponding setup can be found in repository, whereas installation is done via the following commands:</p> <pre><code>cd openshift_k8s\nhelm install -f chart-values.yml pgwatch ./helm-chart\n</code></pre> <p>Please have a look at <code>helm-chart/values.yaml</code> to get additional information of configurable options.</p>"},{"location":"long_term_installations.html","title":"Long term installations","text":"<p>For long term pgwatch setups the main challenge is to keep the software up-to-date to guarantee stable operation and also to make sure that all DB-s are under monitoring.</p>"},{"location":"long_term_installations.html#keeping-inventory-in-sync","title":"Keeping inventory in sync","text":"<p>Adding new DBs to monitoring and removing those shut down, can become a problem if teams are big, databases are many, and it's done per hand (common for on-premise, non-orchestrated deployments). To combat that, the most typical approach would be to write some script or Cronjob that parses the company's internal inventory database, files or endpoints and translate changes to according CRUD operations on the pgwatch.monitored_db table directly.</p> <p>One could also use the REST API for that purpose.</p> <p>If pgwatch configuration is kept in YAML files, it should be also relatively easy to automate the maintenance as the configuration can be organized so that one file represent a single monitoring entry, i.e. the <code>--config</code> parameter can also refer to a folder of YAML files.</p>"},{"location":"long_term_installations.html#updating-the-pgwatch-collector","title":"Updating the pgwatch collector","text":"<p>The pgwatch metrics gathering daemon is the core component of the solution alas the most critical one. So it's definitely recommended to update it at least once per year or minimally when some freshly released Postgres major version instances are added to monitoring. New Postgres versions don't necessary mean that something will break, but you'll be missing some newly added metrics, plus the occasional optimizations. See the upgrading chapter for details, but basically the process is very similar to initial installation as the collector doesn't have any state on its own - it's just on binary program.</p>"},{"location":"long_term_installations.html#metrics-maintenance","title":"Metrics maintenance","text":"<p>Metric definition SQL-s are regularly corrected as suggestions / improvements come in and also new ones are added to cover latest Postgres versions, so would make sense to refresh them 1-2x per year.</p> <p>If using a YAML based config, just installing newer pre-built RPM / DEB packages will do the trick automatically (built-in metrics at /etc/pgwatch/metrics will be refreshed) but for Config DB based setups you'd need to follow a simple process described here.</p>"},{"location":"long_term_installations.html#dashboard-maintenance","title":"Dashboard maintenance","text":"<p>Same as with metrics, also the built-in Grafana dashboards are being actively updates, so would make sense to refresh them occasionally also. The bulk delete / import scripts can be found here or you could also manually just re-import some dashboards of interest from JSON files in [/etc/pgwatch/grafana-dashboards] folder or from Github.</p> <p>Info</p> <p>Notable new dashboards are usually listed also in release notes and most dashboards also have a sample screenshots available.</p>"},{"location":"long_term_installations.html#storage-monitoring","title":"Storage monitoring","text":"<p>In addition to all that you should at least initially periodically monitor the metrics DB size...as it can grow quite a lot (especially when using Postgres for storage) when the monitored databases have hundreds of tables / indexes and if a lot of unique SQL-s are used and pg_stat_statements monitoring is enabled. If the storage grows too fast, one can increase the metric intervals (especially for \"table_stats\", \"index_stats\" and \"stat_statements\") or decrease the data retention periods via --pg-retention-days or --iretentiondays params.</p>"},{"location":"metric_definitions.html","title":"Metric definitions","text":""},{"location":"metric_definitions.html#whats-is-metric","title":"Whats is metric?","text":"<p>Metrics are named SQL queries that return a timestamp and pretty much anything else you find useful. Most metrics have many different query text versions for different target PostgreSQL versions, also optionally taking into account primary / replica state and as of v1.8 also versions of installed extensions.</p> <pre><code>-- a sample metric\nSELECT\n  (extract(epoch from now()) * 1e9)::int8 as epoch_ns,\n  extract(epoch from (now() - pg_postmaster_start_time()))::int8 as postmaster_uptime_s,\n  case when pg_is_in_recovery() then 1 else 0 end as in_recovery_int;\n</code></pre> <p>Correct version of the metric definition will be chosen automatically by regularly connecting to the target database and checking the Postgres version, recovery state, and if the monitoring user is a superuser or not. </p>"},{"location":"metric_definitions.html#built-in-metrics-and-presets","title":"Built-in metrics and presets","text":"<p>There's a good set of pre-defined metrics &amp; metric configs provided by the pgwatch project to cover all typical needs, but when monitoring hundreds of hosts you'd typically want to develop some custom Preset Configs or at least adjust the metric fetching intervals according to your monitoring goals.</p> <p>Some things to note about the built-in metrics:</p> <ul> <li>Only a half of them are included in the Preset configs and are     ready for direct usage. The rest need some extra extensions or     privileges, OS level tool installations etc. To see what's possible     just browse the sample metrics.</li> <li>Some builtin metrics are marked to be only executed when server is a     primary or conversely, a standby. The flags can be inspected / set     on the Web UI Metrics tab or in YAML mode by suffixing the metric     definition with \"standby\" or \"master\". </li> <li>There are a couple of special preset metrics that have some     non-standard behaviour attached to them, e.g. change_events, recommendations,      server_log_event_counts, instance_up.</li> </ul>"},{"location":"metric_definitions.html#change_events","title":"change_events","text":"<p>The \"change_events\" built-in metric, tracking DDL &amp; config changes, uses internally some other \"*_hashes\" metrics which are not meant to be used on their own. Such metrics are described also accordingly on the Web UI /metrics page and they should not be removed.</p>"},{"location":"metric_definitions.html#recommendations","title":"recommendations","text":"<p>When enabled (i.e. <code>interval &gt; 0</code>), this metric will find all other metrics starting with <code>reco_*</code> and execute those queries. The purpose of the metric is to spot some performance, security and other \"best practices\" violations. Users can add new <code>reco_*</code> queries freely.</p>"},{"location":"metric_definitions.html#server_log_event_counts","title":"server_log_event_counts","text":"<p>This enables Postgres server log \"tailing\" for errors. Can't be used for \"pull\" setups though unless the DB logs are somehow mounted / copied over, as real file access is needed. See the Log parsing chapter for details.</p>"},{"location":"metric_definitions.html#instance_up","title":"instance_up","text":"<p>For normal metrics there will be no data rows stored if the DB is not reachable, but for this one there will be a 0 stored for the \"is_up\" column that under normal operations would always be 1. This metric can be used to calculate some \"uptime\" SLA indicator for example.</p>"},{"location":"metric_definitions.html#archiver","title":"archiver","text":"<p>This metric retrieves key statistics from the PostgreSQL <code>pg_stat_archiver</code> view, providing insights into the status of WAL file archiving.  It returns the total number of successfully archived files and failed archiving attempts. Additionally, it identifies if the most recent attempt  resulted in a failure and calculates how many seconds have passed since the last failure. The metric only considers data if WAL archiving is  enabled in the system, helping administrators monitor and diagnose issues related to the archiving process.</p>"},{"location":"metric_definitions.html#backends","title":"backends","text":"<p>This metric gathers detailed information from the PostgreSQL <code>pg_stat_activity</code> view, providing an overview of the current session and activity  state for the database. It tracks the total number of client backends, active sessions, idle sessions, sessions waiting on locks, and background  workers. The metric also calculates statistics on blocked sessions, longest waiting times, average and longest session durations, transaction times,  and query durations. Additionally, it monitors autovacuum worker activity and provides the age of the oldest transaction (measured by <code>xmin</code>). This  metric helps administrators monitor session states, detect bottlenecks, and ensure the system is within its connection limits, providing visibility  into database performance and contention.</p>"},{"location":"metric_definitions.html#bgwriter","title":"bgwriter","text":"<p>This metric retrieves statistics from the <code>pg_stat_bgwriter</code> view, providing information about the background writer process in PostgreSQL. It reports the number of buffers that have been cleaned (written to disk) by the background writer, how many times buffers were written because the background writer reached the maximum limit (<code>maxwritten_clean</code>), and the total number of buffers allocated. Additionally, it calculates the time in seconds since the last reset of these statistics. This metric helps monitor the efficiency and behavior of PostgreSQL's background writer, which plays a crucial role in managing I/O by writing modified buffers to disk, thus helping to ensure smooth database performance.</p>"},{"location":"metric_definitions.html#blocking_locks","title":"blocking_locks","text":"<p>This metric provides information about lock contention in PostgreSQL by identifying sessions that are waiting for locks and the sessions holding those locks.  It captures details from the <code>pg_locks</code> view and the <code>pg_stat_activity</code> view to highlight the interactions between the waiting and blocking sessions. The result helps identify which queries are causing delays due to lock contention, the type of locks involved, and the users or sessions responsible for holding or  waiting on locks. This metric is useful for diagnosing performance bottlenecks related to database locking.</p>"},{"location":"metric_definitions.html#checkpointer","title":"checkpointer","text":"<p>This metric provides insights into the activity and performance of PostgreSQL's checkpointer process, which ensures that modified data pages are regularly written to disk to maintain consistency. It tracks the number of checkpoints that have been triggered either by the system's timing or by specific requests, as well as how many restart points have been completed in standby environments. Additionally, it measures the time spent writing and synchronizing buffers to disk, the total number of buffers written, and how long it has been since the last reset of these statistics. This metric helps administrators understand how efficiently the system is handling checkpoints and whether there might be I/O performance issues related to the frequency or duration of checkpoint operations.</p>"},{"location":"metric_definitions.html#db_stats","title":"db_stats","text":"<p>This metric provides a comprehensive overview of various performance and health statistics for the current PostgreSQL database. It tracks key metrics such as the number of active database connections (<code>numbackends</code>), transaction statistics (committed, rolled back), block I/O (blocks read and hit in the cache), and tuple operations (rows returned, fetched, inserted, updated, deleted). Additionally, it monitors conflicts, temporary file usage, deadlocks, and block read/write times.</p> <p>The metric also includes system uptime by calculating how long the PostgreSQL <code>postmaster</code> process has been running and tracks checksum failures and the time since the last checksum failure. It identifies if the database is in recovery mode, retrieves the system identifier, and tracks session-related statistics such as total session time, active time, idle-in-transaction time, and sessions that were abandoned, fatal, or killed.</p> <p>Lastly, it monitors the number of invalid indexes that are not currently being rebuilt. This metric helps database administrators gain insights into overall database performance, transaction behavior, session activity, and potential index-related issues, which are critical for efficient database management and troubleshooting.</p>"},{"location":"metric_definitions.html#wal","title":"wal","text":"<p>This metric tracks key information about the PostgreSQL system's write-ahead logging (WAL) and recovery state. It calculates the current WAL location, showing how far the system has progressed in terms of WAL writing or replaying if in recovery mode. The metric also indicates whether the database is in recovery, monitors the system's uptime since the <code>postmaster</code> process started, and provides the system's unique identifier. Additionally, it retrieves the current timeline, which is essential for tracking the state of the WAL log and recovery process. This metric helps administrators monitor database health, especially in terms of recovery and WAL operations.</p>"},{"location":"metric_definitions.html#locks","title":"locks","text":"<p>This metric identifies lock contention in the PostgreSQL database by tracking sessions that are waiting for locks and the corresponding sessions holding those locks. It examines active queries in the current database and captures detailed information about both the waiting and blocking sessions. For each waiting session, it records the lock type, user, lock mode, and the query being executed, as well as the table involved. Similarly, for the session holding the lock, it captures the same details. This helps database administrators identify queries that are causing delays due to lock contention, enabling them to troubleshoot performance issues and optimize query execution.</p>"},{"location":"metric_definitions.html#kpi","title":"kpi","text":"<p>This metric provides a detailed overview of PostgreSQL database performance and activity. It tracks the current WAL (Write-Ahead Log) location, the number of active and blocked backends, and the oldest transaction time. It calculates the total transaction rate (TPS) by summing committed and rolled-back transactions, as well as specific statistics on table and index performance, such as the number of sequential scans on tables larger than 10MB and the number of function calls. </p> <p>Additionally, the metric tracks block read and write times, the amount of temporary bytes used, deadlocks, and whether the database is in recovery mode. Finally, it calculates the uptime of the PostgreSQL <code>postmaster</code> process. This information helps administrators monitor and manage system performance, detect potential bottlenecks, and optimize query and transaction behavior.</p>"},{"location":"metric_definitions.html#stat_statements","title":"stat_statements","text":"<p>This metric provides detailed statistics about the performance and resource usage of SQL queries executed on the PostgreSQL database. It collects data from the <code>pg_stat_statements</code> view, focusing on queries that have been executed more than five times and have significant execution time (greater than 5 milliseconds). It aggregates important performance metrics for each query, such as:</p> <ul> <li>Execution metrics: Total number of executions (<code>calls</code>), total execution time, and total planning time.</li> <li>I/O metrics: Blocks read and written (both shared and temporary), blocks dirtied, and associated read/write times.</li> <li>WAL metrics: WAL (Write-Ahead Log) bytes generated and the number of WAL full page images (FPI).</li> <li>User activity: The users who executed the queries and a sample of the query text.</li> </ul> <p>The metric ranks queries based on different performance factors, including execution time, number of calls, block reads/writes, and temporary block usage, and it limits the results to the top 100 queries in each category. This helps administrators identify resource-intensive queries, optimize database performance, and improve query efficiency by focusing on those that consume the most I/O or take the longest to execute.</p>"},{"location":"metric_definitions.html#table_stats","title":"table_stats","text":"<p>This metric collects and summarizes detailed information about table sizes, table activity, and maintenance operations in PostgreSQL. It tracks both individual tables and partitioned tables, including their root partitions. The metric calculates the size of each table (in bytes), as well as other key statistics like sequential scans, index scans, tuples inserted, updated, or deleted, and the number of live and dead tuples. It also tracks maintenance operations like vacuum and analyze runs, as well as whether autovacuum is disabled for specific tables.</p> <p>For partitioned tables, the metric aggregates the statistics across all partitions and provides a summary of the partitioned table as a whole, marking it as the root partition. Additionally, it calculates the time since the last vacuum and analyze operations and captures transaction freeze age for each table, which helps monitor when a table might need a vacuum to prevent transaction wraparound.</p> <p>By focusing on tables larger than 10MB and ignoring temporary and system tables, this metric helps database administrators monitor the largest and most active tables in their database, ensuring that maintenance operations like vacuum and analyze are running effectively and identifying tables that may be contributing to performance bottlenecks due to size or activity.</p>"},{"location":"metric_definitions.html#custom-metrics","title":"Custom metrics","text":"<p>For defining metrics definitions you should adhere to a couple of basic concepts:</p> <ul> <li> <p>Every metric query should have an <code>epoch_ns</code> (nanoseconds since     epoch column to record the metrics reading time. If the column is     not there, things will still work but server timestamp of the     metrics gathering daemon will be used, some a small loss (assuming     intra-datacenter monitoring with little lag) of precision occurs.</p> </li> <li> <p>Queries should only return text, integer, boolean or floating point     (a.k.a. double precision) Postgres data types. Note that columns     with NULL values are not stored at all in the data layer as it's a     bit bothersome to work with NULLs!</p> </li> <li> <p>Column names should be descriptive enough so that they're     self-explanatory, but not over long as it costs also storage</p> </li> <li> <p>Metric queries should execute fast - at least below the selected     Statement timeout (default 5s)</p> </li> <li> <p>Columns can be optionally \"tagged\" by prefixing them with     <code>tag_</code>. By doing this, the column data will be indexed by the     Postgres giving following advantages:</p> <ul> <li>Sophisticated auto-discovery support for indexed keys/values,     when building charts with Grafana.</li> <li>Faster queries for queries on those columns.</li> </ul> </li> <li> <p>All fetched metric rows can also be \"prettyfied\" with any custom     static key-value data, per host. To enable use the \"Custom tags\"     Web UI field for the monitored DB entry or \"custom_tags\" YAML     field. Note that this works per host and applies to all metrics.</p> </li> <li> <p>For Prometheus the numerical columns are by default mapped to a     Value Type of \"Counter\" (as most Statistics Collector columns are     cumulative), but when this is not the case and the column is a     \"Gauge\" then according column attributes should be declared. See     below section on column attributes for details.</p> </li> <li> <p>For Prometheus all text fields will be turned into tags / labels as     only floats can be stored!</p> </li> </ul>"},{"location":"metric_definitions.html#adding-and-using-a-custom-metric","title":"Adding and using a custom metric","text":""},{"location":"metric_definitions.html#for-config-db-based-setups","title":"For Config DB based setups:","text":"<ol> <li>Go to the Web UI \"Metric definitions\" page and scroll to the     bottom.</li> <li>Fill the template - pick a name for your metric, select minimum     supported PostgreSQL version and insert the query text and any     extra attributes if any (see below for options). Hit the \"New\"     button to store.</li> <li>Activate the newly added metric by including it in some existing     Preset Config (listed on top of the page) or add it directly in     JSON form, together with an interval, into the \"Custom metrics     config\" filed on the \"DBs\" page.</li> </ol>"},{"location":"metric_definitions.html#for-yaml-based-setups","title":"For YAML based setups:","text":"<ol> <li>Create a new folder for the metric under     <code>/etc/pgwatch/metrics</code>. The folder name will be the metric's     name, so choose wisely.</li> <li> <p>Create a new subfolder for each \"minimally supported Postgres     version and insert the metric's SQL definition into a file     named \"metric.sql\". </p> <p>Notice</p> <p>Note the \"minimally supported\" part - i.e. if your query will work from version v11.0 to v17 then you only need one entry called \"11\". If there was a breaking change in the internal catalogs at v13 so that the query stopped working, you need a new entry named \"13\" that will be used for all versions above v13.</p> </li> <li> <p>Activate the newly added metric by including it in some existing     Preset Config or add     it directly to the YAML config \"custom_metrics\" section.</p> </li> </ol>"},{"location":"metric_definitions.html#metric-attributes","title":"Metric attributes","text":"<p>The ehaviour of plain metrics can be extended with a set of attributes that will modify the gathering in some way. The attributes are stored in YAML files called *metric_attrs.yaml\" in a metrics root directory or in the <code>metric_attribute</code> Config DB table.</p> <p>Currently supported attributes are:</p> <ul> <li> <p>is_instance_level</p> <p>Enables caching, i.e. sharing of metric data between various databases of a single instance to reduce load on the monitored server.</p> <pre><code>        wal:\n            sqls:\n                11: |\n                    select /* pgwatch_generated */\n                    ...\n            gauges:\n                - '*'\n            is_instance_level: true\n</code></pre> </li> <li> <p>statement_timeout_seconds</p> <p>Enables to override the default 'per monitored DB' statement timeouts on metric level.</p> </li> <li> <p>metric_storage_name</p> <p>Enables dynamic \"renaming\" of metrics at storage level, i.e. declaring almost similar metrics with different names but the data will be stored under one metric. Currently used (for out-of-the box metrics) only for the <code>stat_statements_no_query_text</code> metric, to not to store actual query texts from the \"pg_stat_statements\" extension for more security sensitive instances.</p> </li> <li> <p>extension_version_based_overrides</p> <p>Enables to \"switch out\" the query text from some other metric based on some specific extension version. See 'reco_add_index' for an example definition.</p> </li> <li> <p>disabled_days</p> <p>Enables to \"pause\" metric gathering on specified days. See <code>metric_attrs.yaml</code> for \"wal\" for an example.</p> </li> <li> <p>disabled_times</p> <p>Enables to \"pause\" metric gathering on specified time intervals. e.g. \"09:00-17:00\" for business hours. Note that if time zone is not specified the server time of the gather daemon is used. disabled_days / disabled_times can also be defined both on metric and host (host_attrs) level.</p> </li> </ul>"},{"location":"metric_definitions.html#column-attributes","title":"Column attributes","text":"<p>Besides the _tag column prefix modifier, it's also possible to modify the output of certain columns via a few attributes. It's only relevant for Prometheus output though currently, to set the correct data types in the output description, which is generally considered a nice-to-have thing anyways. For YAML based setups this means adding a \"column_attrs.yaml\" file in the metric's top folder and for Config DB based setup an according \"column_attrs\" JSON column should be filled via the Web UI.</p> <p>Supported column attributes:</p> <ul> <li> <p>gauges</p> <p>Describe the mentioned output columns as of TYPE gauge, i.e. the value can change any time in any direction. Default TYPE for pgwatch is counter.</p> <pre><code>    table_stats_approx:\n        sqls:\n            11: |\n                ...\n        gauges:\n            - table_size_b\n            - total_relation_size_b\n            - toast_size_b\n            - seconds_since_last_vacuum\n            - seconds_since_last_analyze\n            - n_live_tup\n            - n_dead_tup\n        metric_storage_name: table_stats\n</code></pre> </li> </ul>"},{"location":"metric_definitions.html#adding-metric-fetching-helpers","title":"Adding metric fetching helpers","text":"<p>As mentioned in Helper Functions section, Postgres knows very little about the Operating System that it's running on, so in some (most) cases it might be advantageous to also monitor some basic OS statistics together with the PostgreSQL ones, to get a better head start when troubleshooting performance problems. But as setup of such OS tools and linking the gathered data is not always trivial, pgwatch has a system of helpers for fetching such data.</p> <p>One can invent and install such helpers on the monitored databases freely to expose any information needed (backup status etc) via Python, or any other PL-language supported by Postgres, and then add according metrics similarly to any normal Postgres-native metrics.</p>"},{"location":"preparing_databases.html","title":"Preparing databases for monitoring","text":""},{"location":"preparing_databases.html#effects-of-monitoring","title":"Effects of monitoring","text":"<ul> <li>Although the \"Observer effect\" applies also for pgwatch, no     noticeable impact for the monitored DB is expected when using     Preset configs settings, and given that there is some normal load     on the server anyways and the DB doesn't have thousands of tables.     For some metrics though can happen that the metric reading query     (notably \"stat_statements\" and \"table_stats\") takes some tens of     milliseconds, which might be more than an average application query.</li> <li>At any time maximally 2 metric fetching queries can run in parallel     on any monitored DBs. This can be changed by recompiling     (MAX_PG_CONNECTIONS_PER_MONITORED_DB variable) the gatherer.</li> <li>Default Postgres statement     timeout     is 5s for entries inserted via the Web UI / database directly.</li> </ul>"},{"location":"preparing_databases.html#basic-preparations","title":"Basic preparations","text":"<p>As a base requirement you'll need a login user (non-superuser suggested) for connecting to your server and fetching metrics.</p> <p>Though theoretically you can use any username you like, but if not using \"pgwatch\" you need to adjust the \"helper\" creation SQL scripts (see below for explanation) accordingly, as in those by default the \"pgwatch\" will be granted execute privileges.</p> <pre><code>CREATE ROLE pgwatch WITH LOGIN PASSWORD 'secret';\n-- For critical databases it might make sense to ensure that the user account\n-- used for monitoring can only open a limited number of connections\n-- (there are according checks in code, but multiple instances might be launched)\nALTER ROLE pgwatch CONNECTION LIMIT 3;\nGRANT pg_monitor TO pgwatch;\nGRANT CONNECT ON DATABASE mydb TO pgwatch;\nGRANT EXECUTE ON FUNCTION pg_stat_file(text) to pgwatch; -- for wal_size metric\n</code></pre> <p>For most monitored databases it's extremely beneficial (to troubleshooting performance issues) to also activate the pg_stat_statements extension which will give us exact \"per query\" performance aggregates and also enables to calculate how many queries are executed per second for example. In pgwatch context it powers the \"Stat statements Top\" dashboard and many other panels of other dashboards. For additional troubleshooting benefits also the track_io_timing setting should be enabled.</p> <ol> <li> <p>Make sure the Postgres contrib package is installed (should be     installed automatically together with the Postgres server package on     Debian based systems).</p> <ul> <li>On RedHat / Centos: <code>yum install -y postgresqlXY-contrib</code></li> <li>On Debian / Ubuntu: <code>apt install postgresql-contrib</code></li> </ul> </li> <li> <p>Add <code>pg_stat_statements</code> to your server config (postgresql.conf) and     restart the server.</p> <pre><code>shared_preload_libraries = 'pg_stat_statements'\ntrack_io_timing = on\n</code></pre> </li> <li> <p>After restarting activate the extension in the monitored DB. Assumes     Postgres superuser.</p> <pre><code>psql -c \"CREATE EXTENSION IF NOT EXISTS pg_stat_statements\"\n</code></pre> </li> </ol>"},{"location":"preparing_databases.html#rolling-out-helper-functions","title":"Rolling out helper functions","text":"<p>Helper functions in pgwatch context are standard Postgres stored procedures, running under <code>SECURITY DEFINER</code> privileges. Via such wrapper functions one can do controlled privilege escalation - i.e. to give access to protected Postgres metrics (like active session details, \"per query\" statistics) or even OS-level metrics, to normal unprivileged users, like the pgwatch monitoring role.</p> <p>If using a superuser login (recommended only for local \"push\" setups) you have full access to all Postgres metrics and would need helpers only for OS remote statistics. For local (push) setups as of pgwatch version 1.8.4 the most typical OS metrics are covered by the <code>--direct-os-stats</code> flag, explained below.</p> <p>For unprivileged monitoring users it is highly recommended to take these additional steps on the \"to be monitored\" database to get maximum value out of pgwatch in the long run. Without these additional steps, you lose though about 10-15% of built-in metrics, which might not be too tragical nevertheless. For that use case there's also a preset config named \"unprivileged\".</p> <p>When monitoring v10+ servers then the built-in pg_monitor system role is recommended for the monitoring user, which almost substitutes superuser privileges for monitoring purposes in a safe way.</p>"},{"location":"preparing_databases.html#rolling-out-common-helpers","title":"Rolling out common helpers","text":"<p>For completely unprivileged monitoring users the following helpers are recommended to make good use of the default \"exhaustive\" Preset Config:</p> <pre><code>export PGUSER=superuser\npsql -f /etc/pgwatch/metrics/00_helpers/get_stat_activity/$pgver/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_stat_replication/$pgver/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_wal_size/$pgver/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_stat_statements/$pgver/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_sequences/$pgver/metric.sql mydb\n</code></pre> <p>Note that there might not be an exact Postgres version match for helper definitions - then replace \\$pgver with the previous available version number below your server's Postgres version number.</p> <p>Also note that as of v1.8.1 some helpers definition SQL-s scripts (like for \"get_stat_statements\") will inspect also the \"search_path\" and by default will not install into schemas that have PUBLIC CREATE privileges, like the \"public\" schema by default has!</p> <p>Also when rolling out helpers make sure the <code>search_path</code> is at defaults or set so that it's also accessible for the monitoring role as currently neither helpers nor metric definition SQL-s don't assume any particualar schema and depend on the <code>search_path</code> including everything needed.</p> <p>For more detailed statistics (OS monitoring, table bloat, WAL size, etc) it is recommended to install also all other helpers found from the <code>/etc/pgwatch/metrics/00_helpers</code> folder or do it automatically by using the rollout_helper.py script found in the 00_helpers folder.</p> <p>As of v1.6.0 though helpers are not needed for Postgres-native metrics (e.g. WAL size) if a privileged user (superuser or pg_monitor GRANT) is used, as pgwatch now supports having 2 SQL definitions for each metric - \"normal / unprivileged\" and \"privileged\" / \"superuser\". In the file system /etc/pgwatch/metrics such \"privileged\" access definitions will have a \"_su\" added to the file name.</p>"},{"location":"preparing_databases.html#automatic-rollout-of-helpers","title":"Automatic rollout of helpers","text":"<p>pgwatch can roll out helpers also automatically on the monitored DB. This requires superuser privileges and a configuration attribute for the monitored DB. In YAML config mode it's called is_superuser, in Config DB md_is_superuser, in the Web UI one can tick the \"Auto-create helpers\" checkbox.</p> <p>After the automatic rollout it's still generally recommended to remove the superuser privileges from the monitoring role, which now should have GRANT-s to all automatically created helper functions. Note though that all created helpers will not be immediately usable as some are for special purposes and need additional dependencies.</p> <p>A hint: if it can be foreseen that a lot of databases will be created on some instance (generally not a good idea though) it might be a good idea to roll out the helpers directly in the template1 database - so that all newly created databases will get them automatically.</p>"},{"location":"preparing_databases.html#plpython-helpers","title":"PL/Python helpers","text":"<p>PostgreSQL in general is implemented in such a way that it does not know too much about the operation system that it is running on. This is a good thing for portability but can be somewhat limiting for monitoring, especially when there is no system monitoring framework in place or the data is not conveniently accessible together with metrics gathered from Postgres. To overcome this problem, users can also choose to install helpers extracting OS metrics like CPU, RAM usage, etc so that this data is stored together with Postgres-native metrics for easier graphing / correlation / alerting. This also enable to be totally independent of any System Monitoring tools like Zabbix, etc, with the downside that everything is gathered over Postgres connections so that when Postgres is down no OS metrics can be gathered also. Since v1.8.4 though the latter problem can be reduced for local \"push\" based setups via the <code>--direct-os-stats</code> option plus according metrics configuration (e.g. the \"full\" preset).</p> <p>Note though that PL/Python is usually disabled by DB-as-a-service providers like AWS RDS for security reasons.</p> <pre><code># first install the Python bindings for Postgres\napt install postgresql-plpython3-XY\n# yum install postgresqlXY-plpython3\n\npsql -c \"CREATE EXTENSION plpython3u\"\npsql -f /etc/pgwatch/metrics/00_helpers/get_load_average/9.1/metric.sql mydb\n\n# psutil helpers are only needed when full set of common OS metrics is wanted\napt install python3-psutil\npsql -f /etc/pgwatch/metrics/00_helpers/get_psutil_cpu/9.1/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_psutil_mem/9.1/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_psutil_disk/9.1/metric.sql mydb\npsql -f /etc/pgwatch/metrics/00_helpers/get_psutil_disk_io_total/9.1/metric.sql mydb\n</code></pre> <p>Note that we're assuming here that we're on a modern Linux system with Python 3 as default. For older systems Python 3 might not be an option though, so you need to change plpython3u to plpythonu and also do the same replace inside the code of the actual helper functions! Here the rollout_helper.py script with it's <code>--python2</code> flag can be helpful again.</p>"},{"location":"preparing_databases.html#notice-on-using-metric-fetching-helpers","title":"Notice on using metric fetching helpers","text":"<ul> <li>Starting from Postgres v10 helpers are mostly not needed (only for     PL/Python ones getting OS statistics) - there are available some     special monitoring roles like <code>pg_monitor</code>, that are exactly meant     to be used for such cases where we want to give access to all     Statistics Collector views without any other \"superuser     behaviour\". See     here     for documentation on such special system roles. Note that currently     most out-of-the-box metrics first rely on the helpers as v10 is     relatively new still, and only when fetching fails, direct access     with the \"Privileged SQL\" is tried.</li> <li>For gathering OS statistics (CPU, IO, disk) there are helpers and     metrics provided, based on the \"psutil\" Python package... but from     user reports seems the package behaviour differentiates slightly     based on the Linux distro / Kernel version used, so small     adjustments might be needed there (e.g. to remove a non-existent     column). Minimum usable Kernel version required is 3.3.</li> <li>When running the gatherer locally, i.e. having a \"push\" based     configuration, the metric fetching helpers are mostly not needed     as superuser can be used in a safe way and starting from v1.8.4 one     can also enable the <code>--direct-os-stats</code> parameter to signal that     we can fetch the data for the default <code>psutil*</code> metrics     directly from OS counters. If direct OS fetching fails though, the     fallback is still to try via PL/Python wrappers.</li> <li>In rare cases when some \"helpers\" have been installed, and when     doing a binary PostgreSQL upgrade at some later point in time via     <code>pg_upgrade</code>, this could result in error messages     thrown. Then just drop those failing helpers on the \"to be     upgraded\" cluster and re-create them after the upgrade process.</li> </ul>"},{"location":"preparing_databases.html#running-with-developer-credentials","title":"Running with developer credentials","text":"<p>As mentioned above, helper / wrapper functions are not strictly needed, they just provide a bit more information for unprivileged users - thus for developers with no means to install any wrappers as superuser, it's also possible to benefit from pgwatch - for such use cases e.g. the \"unprivileged\" preset metrics profile and the according \"DB overview Unprivileged / Developer\"  are a good starting point as it only assumes existence of <code>pg_stat_statements</code> (which should be available by all cloud providers).</p>"},{"location":"preparing_databases.html#different-source-types-explained","title":"Different source types explained","text":"<p>When adding a new \"to be monitored\" entry a source type needs to be selected. Following types are available:</p>"},{"location":"preparing_databases.html#postgres","title":"postgres","text":"<p>Monitor a single database on a single Postgres instance. When using the Web UI and the \"DB name\" field is left empty, there's as a one time operation where all non-template DB names are fetched, prefixed with \"Unique name\" field value and added to monitoring (if not already monitored). Internally monitoring always happens \"per DB\" not \"per cluster\" though.</p>"},{"location":"preparing_databases.html#postgres-continuous-discovery","title":"postgres-continuous-discovery","text":"<p>Monitor a whole (or subset of DB-s) of Postgres cluster / instance. Host information without a DB name needs to be specified and then the pgwatch daemon will periodically scan the cluster and add any found and not yet monitored DBs to monitoring. In this mode it's also possible to specify regular expressions to include/exclude some database names.</p>"},{"location":"preparing_databases.html#pgbouncer","title":"pgbouncer","text":"<p>Use to track metrics from PgBouncer's <code>SHOW STATS</code> command. In place of the Postgres \"DB name\" the name of the PgBouncer \"pool\" to be monitored must be inserted.</p>"},{"location":"preparing_databases.html#pgpool","title":"pgpool","text":"<p>Use to track joint metrics from Pgpool2's <code>SHOW POOL_NODES</code> and <code>SHOW POOL_PROCESSES</code> commands. Pgpool2 from version 3.0 is supported.</p>"},{"location":"preparing_databases.html#patroni","title":"patroni","text":"<p>Patroni is a HA / cluster manager for Postgres that relies on a DCS (Distributed Consensus Store) to store it's state. Typically in such a setup the nodes come and go and also it should not matter who is currently the master. To make it easier to monitor such dynamic constellations pgwatch supports reading of cluster node info from all supported DCS-s (etcd, Zookeeper, Consul), but currently only for simpler cases with no security applied (which is actually the common case in a trusted environment).</p>"},{"location":"preparing_databases.html#patroni-continuous-discovery","title":"patroni-continuous-discovery","text":"<p>As normal patroni DB type but all DB-s (or only those matching the regex if any provided) are monitored.</p>"},{"location":"preparing_databases.html#patroni-namespace-discovery","title":"patroni-namespace-discovery","text":"<p>Similar to patroni-continuous-discovery but all Patroni scopes (clusters) of an ETCD namespace are automatically monitored. Optionally regexes on database names still apply if provided.</p> <p>Notice</p> <p>All \"continuous\" modes expect access to \"template1\" or \"postgres\" databasess of the specified cluster to determine the database names residing there.</p>"},{"location":"project_background.html","title":"Project background","text":"<p>The pgwatch project got started back in 2016 and released in 2017 initially for internal monitoring needs at Cybertec as all the Open Source PostgreSQL monitoring tools at the time had various limitations like being too slow and invasive to set up or providing a fixed set of visuals and metrics.</p> <p>For more background on the project motivations and design goals see the original series of blogposts announcing the project and the following feature updates released approximately twice per year.</p> <p>Cybertec also provides commercial 9-to-5 and 24/7 support for pgwatch.</p> <ul> <li>Project     announcement</li> <li>Implementation     details</li> <li>Feature pack     1</li> <li>Feature pack     2</li> <li>Feature pack     3</li> <li>Feature pack     4</li> <li>Feature pack     5</li> <li>Feature pack     6</li> <li>Feature pack     7</li> </ul>"},{"location":"project_background.html#project-feedback","title":"Project feedback","text":"<p>For feature requests or troubleshooting assistance please open an issue on project's Github page.</p>"},{"location":"security.html","title":"Security aspects","text":""},{"location":"security.html#general-security-information","title":"General security information","text":"<p>Security can be tightened for most pgwatch components quite granularly, but the default values for the Docker image don't focus on security though but rather on being quickly usable for ad-hoc performance troubleshooting, which is where the roots of pgwatch lie.</p> <p>Some points on security:</p> <ul> <li> <p>The administrative Web UI doesn't have by default any security.     Configurable via env. variables.</p> </li> <li> <p>Viewing Grafana dashboards by default doesn't require login.     Editing needs a password. Configurable via env. variables.</p> </li> <li> <p>Dashboards based on the \"stat_statements\" metric (Stat Statement     Overview / Top) expose actual queries.</p> <p>They should be \"mostly\" stripped of details though and replaced by placeholders by Postgres, but if no risks can be taken such dashboards (or at least according panels) should be deleted. Or as an alternative the <code>stat_statements_no_query_text</code> and <code>pg_stat_statements_calls</code> metrics could be used, which don't store query texts in the first place.</p> </li> <li> <p>Safe certificate connections to Postgres are supported. According      sslmode (verify-ca, verify-full) and cert file paths     need to be specified then in connection string on Web UI \"/dbs\" page      or in the YAML config.</p> </li> <li> <p>Note that although pgwatch can handle password security, in many     cases it's better to still use the standard LibPQ .pgpass file to     store passwords.</p> </li> </ul>"},{"location":"security.html#launching-a-more-secure-docker-container","title":"Launching a more secure Docker container","text":"<p>Some common sense security is built into default Docker images for all components but not actived by default. A sample command to launch pgwatch with following security \"checkpoints\" enabled:</p> <ol> <li>HTTPS for both Grafana and the Web UI with self-signed certificates</li> <li>No anonymous viewing of graphs in Grafana</li> <li>Custom user / password for the Grafana \"admin\" account</li> <li>No anonymous access / editing over the admin Web UI</li> <li>No viewing of internal logs of components running inside Docker</li> <li> <p>Password encryption for connect strings stored in the Config DB</p> <pre><code>docker run --name pw3 -d --restart=unless-stopped \\\n  -p 3000:3000 -p 8080:8080 \\\n  -e PW_GRAFANASSL=1 -e PW_WEBSSL=1 \\\n  -e PW_GRAFANANOANONYMOUS=1 -e PW_GRAFANAUSER=myuser \\\n  -e PW_GRAFANAPASSWORD=mypass \\\n  -e PW_WEBNOANONYMOUS=1 -e PW_WEBNOCOMPONENTLOGS=1 \\\n  -e PW_WEBUSER=myuser -e PW_WEBPASSWORD=mypass \\\n  -e PW_AES_GCM_KEYPHRASE=qwerty \\\n  cybertec/pgwatch\n</code></pre> </li> </ol> <p>For custom installs it's up to the user though. A hint - Docker launcher files can also be inspected to see which config parameters are being touched.</p>"},{"location":"sizing_recommendations.html","title":"Sizing recommendations","text":"<ul> <li> <p>Min 1GB of RAM is required for a Docker setup using Postgres to     store metrics.</p> <p>The gatherer alone needs typically less than 50 MB if the metric  measurements are stored online. Memory consumption will increase a lot when the metrics store is offline though, as then metrics are cached in RAM in ringbuffer style up to a limit of 10k data points (for all databases) and then memory consumption is dependent on how \"wide\" are the metrics gathered.</p> </li> <li> <p>Storage requirements vary a lot and are hard to predict.</p> <p>10GB of disk space should be enough though for monitoring a single DB with \"exhaustive\" preset for 1 month with Postgres storage. 2 weeks is also the default metrics retention policy for Postgres running in Docker (configurable). Depending on the amount of schema objects - tables, indexes, stored procedures and especially on number of unique SQL-s, it could be also much more. If disk size reduction is wanted for PostgreSQL storage then best would be to use the TimescaleDB extension - it has built-in compression and disk footprint is x5 time less than vanila Postgres, while retaining full SQL support.</p> </li> <li> <p>A low-spec (1 vCPU, 2 GB RAM) cloud machine can easily monitor 100     DBs in \"exhaustive\" settings (i.e. almost all metrics are     monitored in 1-2min intervals) without breaking a sweat (\\&lt;20%     load).</p> </li> <li> <p>A single Postgres node should handle thousands of requests per     second.</p> </li> <li> <p>When high metrics write latency is problematic (e.g. using a DBaaS     across the Atlantic) then increasing the default maximum batching     delay of 250ms usually gives good results.     Relevant params: <code>--batching-delay-ms / PW_BATCHING_MAX_DELAY_MS</code>.</p> </li> <li> <p>Note that when monitoring a very large number of databases, it's     possible to \"shard\" / distribute them between many metric     collection instances running on different hosts, via the <code>group</code>     attribute. This requires that some hosts have been assigned a     non-default group identifier, which is just a text field exactly     for this sharding purpose.     Relevant params: <code>--group / PW_GROUP</code>.</p> </li> </ul>"},{"location":"technical_details.html","title":"Technical details","text":"<p>Here are some technical details that might be interesting for those who are planning to use pgwatch for critical monitoring tasks or customize it in some way.</p> <ul> <li> <p>Dynamic management of monitored databases, metrics and their     intervals - no need to restart/redeploy</p> <p>Config DB or YAML / SQL files are scanned every 2 minutes (by default, changeable via <code>--servers-refresh-loop-seconds</code>) and changes are applied dynamically. As common connectivity errors also handled, there should be no need to restart the gatherer \"for fun\". Please always report issues which require restarting.</p> </li> <li> <p>There are some safety features built-in so that monitoring would not     obstruct actual operation of databases</p> <ul> <li>Up to 2 concurrent queries per monitored database (thus more per     cluster) are allowed</li> <li>Configurable statement timeouts per DB</li> <li>SSL connections support for safe over-the-internet monitoring     (use <code>-e PW_WEBSSL=1 -e PW_GRAFANASSL=1</code> when launching     Docker)</li> <li>Optional authentication for the Web UI and Grafana (by default     freely accessible)</li> </ul> </li> <li> <p>Instance-level metrics caching</p> <p>To further reduce load on multi-DB instances, pgwatch can cache the output of metrics that are marked to gather only instance-level data. One such metric is for example \"wal\", and the metric attribute is \"is_instance_level\". Caching will be activated only for continuous source types,  and to a default limit of up to 30 seconds (changeable via the <code>--instance-level-cache-max-seconds</code> param).</p> </li> </ul>"},{"location":"upgrading.html","title":"Upgrading","text":"<p>The pgwatch daemon code doesn't need too much maintenance itself (if you're not interested in new features), but the preset metrics, dashboards and the other components that pgwatch relies, like Grafana, are under very active development and get updates quite regularly so already purely from the security standpoint it would make sense to stay up to date.</p> <p>We also regularly include new component versions in the Docker images after verifying that they work. If using Docker, you could also choose to build your own images any time some new component versions are released, just increment the version numbers in the Dockerfile.</p>"},{"location":"upgrading.html#updating-to-a-newer-docker-version","title":"Updating to a newer Docker version","text":""},{"location":"upgrading.html#without-volumes","title":"Without volumes","text":"<p>If pgwatch container was started in the simplest way possible without volumes, and if previously gathered metrics are not of great importance, and there are no user modified metric or dashboard changes that should be preserved, then the easiest way to get the latest components would be just to launch new container and import the old monitoring config:</p> <pre><code># let's backup up the monitored hosts\npsql -p5432 -U pgwatch -d pgwatch -c \"\\copy monitored_db to 'monitored_db.copy'\"\n\n# stop the old container and start a new one ...\ndocker stop ... &amp;&amp; docker run ....\n\n# import the monitored hosts\npsql -p5432 -U pgwatch -d pgwatch -c \"\\copy monitored_db from 'monitored_db.copy'\"\n</code></pre> <p>If metrics data and other settings like custom dashboards need to be preserved then some more steps are needed, but basically it's about pulling Postgres backups and restoring them into the new container.</p> <p>A tip: to make the restore process easier it would already make sense to mount the host folder with the backups in it on the new container with <code>\"-v \\~/pgwatch_backups:/pgwatch_backups:rw,z\"</code> when starting the Docker image. Otherwise one needs to set up SSH or use something like S3 for example. Also note that port 5432 need to be exposed to take backups outside of Docker for Postgres respectively.</p>"},{"location":"upgrading.html#with-volumes","title":"With volumes","text":"<p>To make updates a bit easier, the preferred way to launch pgwatch should be to use Docker volumes for each individual component - see the Installing using Docker  chapter for details. Then one can just stop the old container and start a new one, re-using the volumes.</p> <p>With some releases though, updating to newer version might additionally still require manual rollout of Config DB schema migrations scripts, so always check the release notes for hints on that or just go to the <code>\"pgwatch/sql/migrations\"</code> folder and execute all SQL scripts that have a higher version than the old pgwatch container. Error messages like will \"missing columns\" or \"wrong datatype\" will also hint at that, after launching with a new image. FYI - such SQL \"patches\" are generally not provided for metric updates, nor dashboard changes and they need to be updated separately.</p>"},{"location":"upgrading.html#updating-without-docker","title":"Updating without Docker","text":"<p>For a custom installation there's quite some freedom in doing updates - as components (Grafana, PostgreSQL) are loosely coupled, they can be updated any time without worrying too much about the other components. Only \"tightly coupled\" components are the pgwatch metrics collector, config DB and the optional Web UI - if the pgwatch config is kept in the database. If YAML based approach is used, then things are even more simple - the pgwatch daemon can be updated any time as YAML schema has default values for everything and there are no other \"tightly coupled\" components like the Web UI.</p>"},{"location":"upgrading.html#updating-grafana","title":"Updating Grafana","text":"<p>The update process for Grafana looks pretty much like the installation so take a look at the according chapter.  If using Grafana's package repository it should happen automatically along with other system packages. Grafana has a built-in database schema migrator, so updating the binaries and restarting is enough.</p>"},{"location":"upgrading.html#updating-grafana-dashboards","title":"Updating Grafana dashboards","text":"<p>There are no update or migration scripts for the built-in Grafana dashboards as it would break possible user applied changes. If you know that there are no user changes, then one can just delete or rename the existing ones in a bulk matter and import the latest JSON definitions. See here for some more advice on how to manage dashboards.</p>"},{"location":"upgrading.html#updating-the-config-metrics-db-version","title":"Updating the config / metrics DB version","text":"<p>Database updates can be quite complex, with many steps, so it makes sense to follow the manufacturer's instructions here.</p> <p>For PostgreSQL one should distinguish between minor version updates and major version upgrades. Minor updates are quite straightforward and problem-free, consisting of running something like:</p> <pre><code>apt update &amp;&amp; apt install postgresql\nsudo systemctl restart postgresql\n</code></pre> <p>For PostgreSQL major version upgrades one should read through the according release notes (e.g. here) and be prepared for the unavoidable downtime.</p>"},{"location":"upgrading.html#updating-the-pgwatch-schema","title":"Updating the pgwatch schema","text":"<p>This is the pgwatch specific part, with some coupling between the following components - Config DB SQL schema, metrics collector, and the optional Web UI.</p> <p>Here one should check from the CHANGELOG if pgwatch schema needs updating. If yes, then manual applying of schema diffs is required before running the new gatherer or Web UI. If no, i.e. no schema changes, all components can be updated independently in random order.</p> <pre><code>pgwatch --config=postgresql://localhost/pgwatch --upgrade\n</code></pre>"},{"location":"upgrading.html#updating-the-metrics-collector","title":"Updating the metrics collector","text":"<p>Compile or install the gatherer from RPM / DEB / tarball packages. See the Custom installation  chapter for details.</p> <p>If using a SystemD service file to auto-start the collector then you might want to also check for possible updates on the template there - <code>/etc/pgwatch/startup-scripts/pgwatch.service</code>.</p>"},{"location":"upgrading.html#updating-metric-definitions","title":"Updating metric definitions","text":"<p>In the YAML mode you always get new SQL definitions for the built-in metrics automatically when refreshing the sources via Github or pre-built packages, but with Config DB approach one needs to do it manually. Given that there are no user added metrics, it's simple enough though - just delete all old ones and re-insert everything from the latest metric definition SQL file.</p> <pre><code>pg_dump -t pgwatch.metric pgwatch &gt; old_metric.sql  # a just-in-case backup\npsql  -c \"truncate pgwatch.metric\" pgwatch\npsql -f /etc/pgwatch/sql/config_store/metric_definitions.sql pgwatch\n</code></pre> <p>Warning</p> <p>If you have added some own custom metrics be sure not to delete or truncate them!</p>"},{"location":"using_managed_services.html","title":"Monitoring managed cloud databases","text":"<p>Although all cloud service providers offer some kind of built-in instrumentation and graphs, they're mostly rather conservative in this are not to consume extra server resources and not to overflow and confuse beginners with too much information. So for advanced troubleshooting it might make sense to gather some additional metrics on your own, especially given that you can also easily add custom business metrics to pgwatch using plain SQL, for example to track the amount of incoming sales orders. Also with pgwatch / Grafana you have more freedom on the visual representation side and access to around 30 prebuilt dashboards and a lot of freedom creating custom alerting rules.</p> <p>The common denominator for all managed cloud services is that they remove / disallow dangerous or potentially dangerous functionalities like file system access and untrusted PL-languages like Python - so you'll lose a small amount of metrics and \"helper functions\" compared to a standard on-site setup described in the <code>previous chapter &lt;preparing_databases&gt;</code>{.interpreted-text role=\"ref\"}. This also means that you will get some errors displayed on some preset dashboards like \"DB overview\" and thus will be better off using a dashboard called \"DB overview Unprivileged\" tailored specially for such a use case.</p> <p>pgwatch has been tested to work with the following managed database services:</p>"},{"location":"using_managed_services.html#google-cloud-sql-for-postgresql","title":"Google Cloud SQL for PostgreSQL","text":"<ul> <li>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the     Stackdriver     data source.</li> <li><code>pg_monitor</code> system role available.</li> <li>pgwatch default preset name: <code>gce</code>.</li> <li>Documentation: https://cloud.google.com/sql/docs/postgres</li> </ul> <p>To get most out pgwatch on GCE you need some additional clicks in the GUI / Cloud Console \"Flags\" section to enable some common PostgreSQL monitoring parameters like <code>track_io_timing</code> and <code>track_functions</code>.</p>"},{"location":"using_managed_services.html#amazon-rds-for-postgresql","title":"Amazon RDS for PostgreSQL","text":"<ul> <li> <p>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the     CloudWatch     data source</p> </li> <li> <p><code>pg_monitor</code> system role available.</p> </li> <li> <p>pgwatch default preset names: <code>rds</code>, <code>aurora</code></p> </li> <li> <p>Documentation:</p> <p>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html</p> </li> </ul> <p>Note that the AWS Aurora PostgreSQL-compatible engine is missing some additional metrics compared to normal RDS.</p>"},{"location":"using_managed_services.html#azure-database-for-postgresql","title":"Azure Database for PostgreSQL","text":"<ul> <li>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the Azure     Monitor     data source</li> <li><code>pg_monitor</code> system role available.</li> <li>pgwatch default preset name: <code>azure</code></li> <li>Documentation: https://docs.microsoft.com/en-us/azure/postgresql/</li> </ul> <p>Surprisingly on Azure some file access functions are whitelisted, thus one can for example use the <code>wal_size</code> metrics.</p> <p>Note</p> <p>By default Azure has pg_stat_statements not fully activated by default, so you need to enable it manually or via the API. Documentation link here.</p>"},{"location":"using_managed_services.html#aiven-for-postgresql","title":"Aiven for PostgreSQL","text":"<p>The Aiven developer documentation contains information on how to monitor PostgreSQL instances running on the Aiven platform with pgwatch.</p>"},{"location":"web_ui.html","title":"The Admin Web UI","text":"<p>If using pgwatch in the centrally managed Config DB way, for easy configuration management (adding databases to monitoring, adding metrics) there is a small Python Web application bundled making use of the CherryPy Web-framework.</p> <p>For mass configuration changes the Web UI has some buttons to disable / enable all hosts for example, but one could technically also log into the configuration database and change the pgwatch.monitored_db table directly.</p> <p>Besides managing the metrics gathering configurations, the two other useful features for the Web UI would be the possibility to look at the logs of the single components and to verify that metrics gathering is working on the \"Stat Statements Overview\" page, which will contact the metrics DB (only Postgres is supported) and present some stats summaries.</p> <p>Default port: 8080</p> <p>Sample screenshot of the Web UI:</p> <p></p>"},{"location":"web_ui.html#web-ui-security","title":"Web UI security","text":"<p>By default the Web UI is not secured - anyone can view and modify the monitoring configuration. If some security is needed though it can be enabled:</p> <ul> <li> <p>HTTPS</p> </li> <li> <p>Password protection is controlled by <code>--web-user</code>, <code>--web-password</code> command-line parameters or     <code>PW_WEBUSER</code>, <code>PW_WEBPASSWORD</code> environmental variables.</p> </li> <li> <p>Note that it's better to use standard LibPQ .pgpass files so     there's no requirement to store any passwords in pgwatch config     database or YAML config file.</p> </li> </ul> <p>For security sensitive environments make sure to always deploy password protection together with SSL, as it uses a standard cookie based techniques vulnerable to snooping / MITM attacks.</p>"},{"location":"web_ui.html#exposing-the-component-logs","title":"Exposing the component logs","text":"<p>When using the Docker images, internal component logs (Postgres, Grafana, Go daemon, Web UI itself) are exposed via the \"/logs\" endpoint. If this is not wanted set the PW_WEBNOCOMPONENTLOGS env. variable. Note that if a working \"/logs\" endpoint is desired also in custom setup mode (non-docker) then some actual code changes are needed to specify where logs of all components are situated - see top of the pgwatch.py file for that.</p>"}]}