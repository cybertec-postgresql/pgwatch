{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>pgwatch3 is a flexible PostgreSQL-specific monitoring solution, relying on Grafana dashboards for the UI part. It supports monitoring of almost all metrics for Postgres versions 9.0 to 13 out of the box and can be easily extended to include custom metrics. At the core of the solution is the metrics gathering daemon written in Go, with many options to configure the details and aggressiveness of monitoring, types of metrics storage and the display the metrics.</p>"},{"location":"#quick-start-with-docker","title":"Quick start with Docker","text":"<p>For the fastest setup experience Docker images are provided via Docker Hub (if new to Docker start here). For custom setups see the Custom installations paragraph below or turn to the pre-built DEB / RPM / Tar packages on the Github Releases page.</p> <p>Launching the latest pgwatch3 Docker image with built-in Postgres metrics storage DB:</p> <pre><code># run the latest Docker image, exposing Grafana on port 3000 and the administrative web UI on 8080\ndocker run -d -p 3000:3000 -p 8080:8080 -e PW3_TESTDB=true --name pw3 cybertec/pgwatch3\n</code></pre> <p>After some minutes you could for example open the DB overview dashboard and start looking at metrics in Grafana. For defining your own dashboards or making changes you need to log in as admin (default user/password: <code>admin/pgwatch3admin</code>).</p> <p>If you don't want to add the <code>\"test\"</code> database (the pgwatch3 configuration DB holding connection strings to monitored DBs and metric definitions) to monitoring, remove the <code>PW3_TESTDB</code> env variable.</p> <p>Also note that for long term production usage with Docker it's highly recommended to use separate volumes for each pgwatch3 component - see here for a better launch example.</p>"},{"location":"#typical-pull-architecture","title":"Typical \"pull\" architecture","text":"<p>To get an idea how pgwatch3 is typically deployed a diagram of the standard Docker image fetching metrics from a set of Postgres databases configured via a configuration DB:</p> <p></p>"},{"location":"#typical-push-architecture","title":"Typical \"push\" architecture","text":"<p>A better fit for very dynamic (Cloud) environments might be a more de-centralized \"push\" approach or just exposing the metrics over a port for remote scraping. In that case the only component required would be the pgwatch3 metrics collection daemon.</p> <p></p>"},{"location":"ENV_VARIABLES/","title":"Available env. variables by components","text":"<p>Some variables influence multiple components. Command line parameters override env. variables (when doing custom deployments).</p>"},{"location":"ENV_VARIABLES/#docker-image-specific","title":"Docker image specific","text":"<ul> <li>PW3_TESTDB When set, the config DB itself will be added to monitoring as \"test\". Default: -</li> </ul>"},{"location":"ENV_VARIABLES/#gatherer-daemon","title":"Gatherer daemon","text":"<p>See <code>pgwatch3 --help</code> output for details.</p>"},{"location":"ENV_VARIABLES/#grafana","title":"Grafana","text":"<ul> <li>PW3_GRAFANANOANONYMOUS Can be set to require login even for viewing dashboards. Default: -</li> <li>PW3_GRAFANAUSER Administrative user. Default: admin</li> <li>PW3_GRAFANAPASSWORD Administrative user password. Default: pgwatch3admin</li> <li>PW3_GRAFANASSL Use SSL. Default: -</li> <li>PW3_GRAFANA_BASEURL For linking to Grafana \"Query details\" dashboard from \"Stat_stmt. overview\". Default: http://0.0.0.0:3000</li> </ul>"},{"location":"advanced_features/","title":"Advanced features","text":"<p>Over the years the core functionality of fetching metrics from a set of plain Postgres DB-s has been extended in many ways to cover some common problem areas like server log monitoring and supporting monitoring of some other popular tools often used together with Postgres, like the PgBouncer connection pooler for example.</p>"},{"location":"advanced_features/#patroni-support","title":"Patroni support","text":"<p>Patroni is a popular Postgres specific HA-cluster manager that makes node management simpler than ever, meaning that everything is dynamic though - cluster members can come and go, making monitoring in the standard way a bit tricky. But luckily Patroni cluster members information is stored in a DCS (Distributed Consensus Store), like etcd, so it can be fetched from there periodically.</p> <p>When 'patroni' is selected as a source type then the usual Postgres  host/port fields should be left empty (\"dbname\" can still filled if only a  specific single database is to be monitored) and instead \"Host config\" JSON field should be filled with DCS address, type and scope (cluster name) information. A sample config (for Config DB based setups) looks like:</p> <pre><code>    {\n      \"dcs_type\": \"etcd\",\n      \"dcs_endpoints\": [\"http://127.0.0.1:2379\"],\n      \"scope\": \"batman\",\n      \"namespace\": \"/service/\"\n    }\n</code></pre> <p>For YAML based setups an example can be found from the instances.yaml file.</p> <p>If Patroni is powered by etcd, then also username, password, ca_file, cert_file, key_file optional security parameters can be defined - other DCS systems are currently only supported without authentication.</p> <p>Also if you don't use the standby nodes actively for queries then it might make sense to decrease the volume of gathered metrics and to disable the monitoring of such nodes with the \"Master mode only?\" checkbox (when using the Web UI) or with only_if_master=true if using a YAML based setup.</p>"},{"location":"advanced_features/#log-parsing","title":"Log parsing","text":"<p>As of v1.7.0 the metrics collector daemon, when running on a DB server (controlled best over a YAML config), has capabilities to parse the database server logs for errors. Out-of-the-box it will though only work when logs are written in CSVLOG format. For other formats user needs to specify a regex that parses out named groups of following fields: database_name, error_severity. See here for an example regex.</p> <p>Note that only the event counts are stored, no error texts, usernames or other infos! Errors are grouped by severity for the monitored DB and for the whole instance. The metric name to enable log parsing is \"server_log_event_counts\". Also note that for auto-detection of log destination / setting to work, the monitoring user needs superuser / pg_monitor privileges - if this is not possible then log settings need to be specified manually under \"Host config\" as seen for example here.</p> <p>Sample configuration if not using CSVLOG logging:</p> <p>On Postgres side (on the monitored DB)</p> <pre><code>    # Debian / Ubuntu default log_line_prefix actually\n    log_line_prefix = '%m [%p] %q%u@%d '\n</code></pre> <p>YAML config (recommended when \"pushing\" metrics from DB nodes to a central metrics DB)</p> <pre><code>    ## logs_glob_path is only needed if the monitoring user is cannot auto-detect it (i.e. not a superuser / pg_monitor role)\n    # logs_glob_path:\n    logs_match_regex: '^(?P&lt;log_time&gt;.*) \\[(?P&lt;process_id&gt;\\d+)\\] (?P&lt;user_name&gt;.*)@(?P&lt;database_name&gt;.*?) (?P&lt;error_severity&gt;.*?): '\n</code></pre> <p>For log parsing to work the metric server_log_event_counts needs to be enabled or a preset config including it used - like the \"full\" preset.</p>"},{"location":"advanced_features/#pgbouncer-support","title":"PgBouncer support","text":"<p>pgwatch3 also supports collecting internal statistics from the PgBouncer connection pooler, via the built-in special \"pgbouncer\" database and the <code>SHOW STATS</code> command. To enable it choose the according DB Type, provide connection info to the pooler port and make sure the pgbouncer_stats metric or \"pgbouncer\" preset config is selected for the host. Note that for the \"DB Name\" field you should insert not \"pgbouncer\" (although this special DB provides all the statistics) but the real name of the pool you wish to monitor or leave it empty to track all pools. In latter case individual pools will be identified / separated via the \"database\" tag.</p> <p>There's also a built-in Grafana dashboard for PgBouncer data, looking like that:</p> <p></p>"},{"location":"advanced_features/#pgpool-ii-support","title":"Pgpool-II support","text":"<p>Quite similar to PgBouncer, also Pgpool offers some statistics on pool performance and status, which might be of interest especially if using the load balancing features. To enable it choose the according DB Type, provide connection info to the pooler port and make sure the pgpool_stats metric / preset config is selected for the host.</p> <p>The built-in Grafana dashboard for Pgpool data looks something like that:</p> <p></p>"},{"location":"advanced_features/#prometheus-scraping","title":"Prometheus scraping","text":"<p>pgwatch3 was originally designed with direct metrics storage in mind, but later also support for externally controlled Prometheus scraping was added. Note that currently though the storage modes are exclusive, i.e. when you enable the Promotheus endpoint (default port 9187) there will be no direct metrics storage.</p> <p>To enable the scraping endpoint set <code>--datastore=prometheus</code> and optionally also <code>--prometheus-port</code>, <code>--prometheus-namespace</code>, <code>--prometheus-listen-addr</code>. Additionally note that you still need to specify some metrics config as usually - only metrics with interval values bigger than zero will be populated on scraping.</p> <p>Currently a few built-in metrics that require some state to be stored between scrapes, e.g. the \"change_events\" metric, will currently be ignored. Also non-numeric data columns will be ignored! Tag columns will be preserved though as Prometheus \"labels\".</p>"},{"location":"advanced_features/#cloud-providers-support","title":"Cloud providers support","text":"<p>Due to popularity of various managed PostgreSQL offerings there's also support for some managed options in sense of Preset Configs, that take into account the fact that on such platforms you get a limited user that doesn't have access to all metrics or some features have just been plain removed. Thus to reduce server log errors and save time on experimenting there are following presets available:</p> <ul> <li>aws - for standard AWS RDS managed PostgreSQL databases</li> <li>aurora - for AWS Aurora managed PostgreSQL service</li> <li>azure - for Azure Database for PostgreSQL managed databases</li> <li>gce - for Google Cloud SQL for PostgreSQL managed databases</li> </ul>"},{"location":"components/","title":"Components","text":"<p>The main development idea around pgwatch3 was to do the minimal work needed and not to reinvent the wheel - meaning that pgwatch3 is mostly just about gluing together already some proven pieces of software for metrics storage and using Grafana for dashboarding. So here a listing of components that can be used to build up a monitoring setup around the pgwatch3 metrics collector. Note that most components are not mandatory and for tasks like metrics storage there are many components to choose from.</p>"},{"location":"components/#the-metrics-gathering-daemon","title":"The metrics gathering daemon","text":"<p>The metrics collector, written in Go, is the only mandatory and most critical component of the whole solution. The main task of the pgwatch3 collector / daemon is pretty simple - reading the configuration and metric defintions, fetching the metrics from the configured databases using the configured connection info and finally storing the metrics to some other database, or just exposing them over a port for scraping in case of Prometheus mode.</p>"},{"location":"components/#configuration","title":"Configuration","text":"<p>The configuration says which databases, how often and with which metrics (SQL-s queries) are to be gathered. There are 3 options to store the configuration:</p> <ul> <li>A PostgreSQL database holding a simple schema with 5 tables;</li> <li>File based approach - YAML config file.</li> </ul>"},{"location":"components/#measurements-storage","title":"Measurements storage","text":"<p>Many options here so that one can for example go for maximum storage effectiveness or pick something where they already know the query language:</p>"},{"location":"components/#postgresql","title":"PostgreSQL","text":"<p>PostgreSQL is a world's most advanced Open Source RDBMS.</p> <p>Postgres storage is based on the JSONB datatype so minimally version 9.4+ is required, but for bigger setups where partitioning is a must, v11+ is needed. Any already existing Postgres database will do the trick, see the Bootstrapping the Metrics DB section for2 details.</p>"},{"location":"components/#timescaledb","title":"TimescaleDB","text":"<p>TimescaleDB is a time-series extension for PostgreSQL.</p> <p>Although technically a plain extension it's often mentioned as a separate database system as it brings custom data compression to the table, enabling huge disk savings over standard Postgres. Note that pgwatch3 does not use Timescale's built-in retention management but a custom version.</p>"},{"location":"components/#prometheus","title":"Prometheus","text":"<p>Prometheus is a time series database and monitoring system.</p> <p>Though Prometheus is not a traditional database system, it's a good choice for monitoring Cloud-like environments as the monitoring targets don't need to know too much about how actual monitoring will be carried out later and also Prometheus has a nice fault-tolerant alerting system for enterprise needs. By default Prometheus is not set up for long term metrics storage!</p>"},{"location":"components/#json-files","title":"JSON files","text":"<p>Plain text files for testing / special use cases.</p>"},{"location":"components/#the-web-ui","title":"The Web UI","text":"<p>The second homebrewn component of the pgwatch3 solution is an optional and relatively simple Web UI for administering details of the monitoring configuration like which databases should be monitored, with which metrics and intervals. Besides that there are some basic overview tables to analyze the gathered data and also possibilities to delete unneeded metric data (when removing a test host for example).</p> <p>Note</p> <p>Note that the Web UI can only be used if storing the configuration in the database (Postgres).</p>"},{"location":"components/#metrics-representation","title":"Metrics representation","text":"<p>Standard pgwatch3 setup uses Grafana for analyzing the gathered metrics data in a visual, point-and-click way. For that a rich set of predefined dashboards for Postgres is provided, that should cover the needs of most users - advanced users would mostly always want to customize some aspects though, so it's not meant as a one-size-fits-all solution. Also as metrics are stored in a DB, they can be visualized or processed in any other way.</p>"},{"location":"components/#component-diagram","title":"Component diagram","text":"<p>Component diagram of a typical setup:</p> <p></p>"},{"location":"components/#component-reuse","title":"Component reuse","text":"<p>All components are loosely coupled, thus for non-pgwatch3 components (pgwatch3 components are only the metrics collector and the optional Web UI) you can decide to make use of an already existing installation of Postgres, Grafana or Prometheus and run additionally just the pgwatch3 collector.</p>"},{"location":"components/#to-use-an-existing-postgres-db-for-storing-the-monitoring-config","title":"To use an existing Postgres DB for storing the monitoring config","text":"<p>Create a new pgwatch3 DB, preferrably also an accroding role who owns it. Then roll out the schema (pgwatch3/sql/config_store/config_store.sql) and set the following parameters when running the image: <code>PW3_PGHOST</code>, <code>PW3_PGPORT</code>, <code>PW3_PGDATABASE</code>, <code>PW3_PGUSER</code>, <code>PW3_PGPASSWORD</code>, <code>PW3_PGSSL</code> (optional).</p>"},{"location":"components/#to-use-an-existing-grafana-installation","title":"To use an existing Grafana installation","text":"<p>Load the pgwatch3 dashboards from grafana_dashboard folder if needed (one can totally define their own) and set the following paramater: <code>PW3_GRAFANA_BASEURL</code>. This parameter only provides correct links to Grafana dashboards from the Web UI. Grafana is the most loosely coupled component for pgwatch3 and basically doesn't have to be used at all. One can make use of the gathered metrics directly over the Postgres (or Graphite) API-s.</p>"},{"location":"components/#to-use-an-existing-postgres-db-for-storing-metrics","title":"To use an existing Postgres DB for storing metrics","text":"<ol> <li>Roll out the metrics storage schema according to instructions     from here.</li> <li> <p>Following parameters need to be set for the gatherer:</p> <ul> <li><code>--datastore=postgres</code> or <code>PW3_DATASTORE=postgres</code></li> <li><code>--pg-metric-store-conn-str=\"postgresql://user:pwd@host:port/db\"</code>     or <code>PW3_PG_METRIC_STORE_CONN_STR=\"...\"</code></li> <li>optionally also adjust the <code>--pg-retention-days</code> parameter. By     default 14 days for Postgres are kept</li> </ul> </li> <li> <p>If using the Web UI also set the datastore parameters     <code>--datastore</code> and <code>--pg-metric-store-conn-str</code> if wanting to     have an option to be able to clean up data also via the UI in a     more targeted way.</p> </li> </ol> <p>When using Postgres metrics storage, the schema rollout script activates \"asynchronous commiting\" feature for the pgwatch3 role in the metrics storage DB by default! If this is not wanted (no metrics can be lost in case of a crash), then re-enstate normal (synchronous) commiting with below query and restart the pgwatch3 agent:</p> <pre><code>ALTER ROLE pgwatch3 IN DATABASE $MY_METRICS_DB SET synchronous_commit TO on;\n</code></pre>"},{"location":"custom_installation/","title":"Custom installation","text":"<p>As described in the Components  chapter, there a couple of ways how to set up up pgwatch3. Two most common ways though are the central Config DB based \"pull\" approach and the YAML file based \"push\" approach, plus Grafana to visualize the gathered metrics.</p>"},{"location":"custom_installation/#config-db-based-setup","title":"Config DB based setup","text":""},{"location":"custom_installation/#overview-of-installation-steps","title":"Overview of installation steps","text":"<ol> <li>Install Postgres or use any available existing instance - v9.4+     required for the config DB and v11+ for the metrics DB.</li> <li>Bootstrap the Config DB.</li> <li>Bootstrap the metrics storage DB (PostgreSQL here).</li> <li>Install pgwatch3 - either from pre-built packages or by compiling     the Go code.</li> <li>Prepare the \"to-be-monitored\" databases for monitoring by creating     a dedicated login role name as a minimum.</li> <li>Optional step - install the administrative Web UI + Python &amp; library     dependencies.</li> <li>Add some databases to the monitoring configuration via the Web UI or     directly in the Config DB.</li> <li>Start the pgwatch3 metrics collection agent and monitor the logs for     any problems.</li> <li>Install and configure Grafana and import the pgwatch3 sample     dashboards to start analyzing the metrics.</li> <li>Make sure that there are auto-start SystemD services for all     components in place and optionally set up also backups.</li> </ol>"},{"location":"custom_installation/#detailed-steps-for-the-config-db-based-pull-approach-with-postgres-metrics-storage","title":"Detailed steps for the Config DB based \"pull\" approach with Postgres metrics storage","text":"<p>Below are sample steps to do a custom install from scratch using Postgres for the pgwatch3 configuration DB, metrics DB and Grafana config DB.</p> <p>All examples here assume Ubuntu as OS - but it's basically the same for RedHat family of operations systems also, minus package installation syntax differences.</p> <ol> <li> <p>Install Postgres</p> <p>Follow the standard Postgres install procedure basically. Use the latest major version available, but minimally v11+ is recommended for the metrics DB due to recent partitioning speedup improvements and also older versions were missing some default JSONB casts so that a few built-in Grafana dashboards need adjusting otherwise.</p> <p>To get the latest Postgres versions, official Postgres PGDG repos are to be preferred over default disto repos. Follow the instructions from:</p> <ul> <li>https://wiki.postgresql.org/wiki/Apt - for Debian / Ubuntu     based systems</li> <li>https://www.postgresql.org/download/linux/redhat/ - for CentOS     / RedHat based systems</li> </ul> </li> <li> <p>Install pgwatch3 - either from pre-built packages or by     compiling the Go code</p> <ul> <li> <p>Using pre-built packages</p> <p>The pre-built DEB / RPM / Tar packages are available on the Github releases page.</p> <pre><code># find out the latest package link and replace below, using v1.8.0 here\nwget https://github.com/cybertec-postgresql/pgwatch3/releases/download/v1.8.0/pgwatch3_v1.8.0-SNAPSHOT-064fdaf_linux_64-bit.deb\nsudo dpkg -i pgwatch3_v1.8.0-SNAPSHOT-064fdaf_linux_64-bit.deb\n</code></pre> </li> <li> <p>Compiling the Go code yourself</p> <p>This method of course is not needed unless dealing with maximum security environments or some slight code changes are required.</p> <ol> <li> <p>Install Go by following the official     instructions</p> </li> <li> <p>Get the pgwatch3 project's code and compile the gatherer     daemon</p> <pre><code>git clone https://github.com/cybertec-postgresql/pgwatch3.git\ncd pgwatch3/src/webui\nyarn install --network-timeout 100000 &amp;&amp; yarn build\ncd ..\ngo build\n</code></pre> <p>After fetching all the Go library dependencies (can take minutes) an executable named \"pgwatch3\" should be generated. Additionally it's a good idea to copy it to <code>/usr/bin/pgwatch3</code>.</p> </li> </ol> </li> <li> <p>Configure a SystemD auto-start service (optional)</p> <p>Sample startup scripts can be found at /etc/pgwatch3/startup-scripts/pgwatch3.service or online here. Note that they are OS agnostic and might need some light adjustment of paths, etc - so always test them out.</p> </li> </ul> </li> <li> <p>Boostrap the config DB</p> <ol> <li> <p>Create a user to \"own\" the <code>pgwatch3</code> schema</p> <p>Typically called <code>pgwatch3</code> but can be anything really, if the schema creation file is adjusted accordingly.</p> <pre><code>psql -c \"create user pgwatch3 password 'xyz'\"\npsql -c \"create database pgwatch3 owner pgwatch3\"\n</code></pre> </li> <li> <p>Roll out the pgwatch3 config schema</p> <p>The schema will most importantly hold connection strings of DB-s to be monitored and the metric definitions.</p> <pre><code># FYI - one could get the below schema files also directly from Github\n# if re-using some existing remote Postgres instance where pgwatch3 was not installed\npsql -f /etc/pgwatch3/sql/config_store/config_store.sql pgwatch3\npsql -f /etc/pgwatch3/sql/config_store/metric_definitions.sql pgwatch3\n</code></pre> </li> </ol> </li> <li> <p>Bootstrap the measurements storage DB</p> <ol> <li> <p>Create a dedicated database for storing metrics and a user to     \"own\" the metrics schema</p> <p>Here again default scripts expect a role named <code>pgwatch3</code> but can be anything if to adjust the scripts.</p> <pre><code>psql -c \"create database pgwatch3_metrics owner pgwatch3\"\n</code></pre> </li> <li> <p>Roll out the pgwatch3 metrics storage schema</p> <p>This is a place to pause and first think how many databases will be monitored, i.e. how much data generated, and based on that one should choose an according metrics storage schema. There are a couple of different options available that are described here in detail, but the gist of it is that you don't want too complex partitioning schemes if you don't have zounds of data and don't need the fastest queries. For a smaller amount of monitored DBs (a couple dozen to a hundred) the default \"metric-time\" is a good choice. For hundreds of databases, aggressive intervals, or long term storage usage of the TimescaleDB extension is recommended.</p> <pre><code>cd /etc/pgwatch3/sql/metric_store\npsql -f roll_out_metric_time.psql pgwatch3_metrics\n</code></pre> <p>Note</p> <p>Default retention for Postgres storage is 2 weeks! To change, use the <code>--pg-retention-days / PW3_PG_RETENTION_DAYS</code> gatherer parameter.</p> </li> </ol> </li> <li> <p>Prepare the \"to-be-monitored\" databases for metrics collection</p> <p>As a minimum we need a plain unprivileged login user. Better though is to grant the user also the <code>pg_monitor</code> system role, available on v10+. Superuser privileges should be normally avoided for obvious reasons of course, but for initial testing in safe environments it can make the initial preparation (automatic helper rollouts) a bit easier still, given superuser privileges are later stripped.</p> <p>To get most out of your metrics some <code>SECURITY DEFINER</code> wrappers functions called \"helpers\" are recommended on the DB-s under monitoring. See the detailed chapter on the \"preparation\" topic here for more details.</p> </li> <li> <p>Configure DB-s and metrics / intervals to be monitored</p> <ul> <li>From the Web UI \"/dbs\" page</li> <li>Via direct inserts into the Config DB <code>pgwatch3.monitored_db</code> table</li> </ul> </li> <li> <p>Start the pgwatch3 metrics collection agent</p> <ol> <li> <p>The gatherer has quite some parameters (use the <code>--help</code> flag     to show them all), but simplest form would be:</p> <pre><code>pgwatch3-daemon \\\n  --host=localhost --user=pgwatch3 --dbname=pgwatch3 \\\n  --datastore=postgres --pg-metric-store-conn-str=postgresql://pgwatch3@localhost:5432/pgwatch3_metrics \\\n  --verbose=info\n</code></pre> <p>Default connections params expect a trusted localhost Config DB setup so mostly the 2nd line is not needed actually.</p> <p>Or via SystemD if set up in previous steps</p> <pre><code>useradd -m -s /bin/bash pgwatch3 # default SystemD templates run under the pgwatch3 user\nsudo systemctl start pgwatch3\nsudo systemctl status pgwatch3\n</code></pre> <p>After initial verification that all works it's usually good idea to set verbosity back to default by removing the verbose flag.</p> <p>Another tip to configure connection strings inside SystemD service files is to use the \"systemd-escape\" utility to escape special characters like spaces etc if using the LibPQ connect string syntax rather than JDBC syntax.</p> </li> <li> <p>Monitor the console or log output for any problems</p> <p>If you see metrics trickling into the \"pgwatch3_metrics\" database (metric names are mapped to table names and tables are auto-created), then congratulations - the deployment is working! When using some more aggressive preset metrics config then there are usually still some errors though, due to the fact that some more extensions or privileges are missing on the monitored database side. See the according chapter here.</p> </li> </ol> <p>Info</p> <p>When you're compiling your own gatherer then the executable file will be named just <code>pgwatch3</code> instead of <code>pgwatch3-daemon</code> to avoid mixups.</p> </li> <li> <p>Install Grafana</p> <ol> <li> <p>Create a Postgres database to hold Grafana internal config, like     dashboards etc</p> <p>Theoretically it's not absolutely required to use Postgres for storing Grafana internal settings / dashboards, but doing so has 2 advantages - you can easily roll out all pgwatch3 built-in dashboards and one can also do remote backups of the Grafana configuration easily.</p> <pre><code>psql -c \"create user pgwatch3_grafana password 'xyz'\"\npsql -c \"create database pgwatch3_grafana owner pgwatch3_grafana\"\n</code></pre> </li> <li> <p>Follow the instructions from     https://grafana.com/docs/grafana/latest/installation/debian/,     basically something like:</p> <pre><code>wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\necho \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\nsudo apt-get update &amp;&amp; sudo apt-get install grafana\n\n# review / change config settings and security, etc\nsudo vi /etc/grafana/grafana.ini\n\n# start and enable auto-start on boot\nsudo systemctl daemon-reload\nsudo systemctl start grafana-server\nsudo systemctl status grafana-server\n</code></pre> <p>Default Grafana port: 3000</p> </li> <li> <p>Configure Grafana config to use our <code>pgwatch3_grafana</code> DB</p> <p>Place something like below in the <code>[database]</code> section of <code>/etc/grafana/grafana.ini</code></p> <pre><code>[database]\ntype = postgres\nhost = my-postgres-db:5432\nname = pgwatch3_grafana\nuser = pgwatch3_grafana\npassword = xyz\n</code></pre> <p>Taking a look at <code>[server], [security]</code> and <code>[auth*]</code> sections is also recommended.</p> </li> <li> <p>Set up the <code>pgwatch3</code> metrics database as the default datasource</p> <p>We need to tell Grafana where our metrics data is located. Add a datasource via the Grafana UI (Admin -&gt; Data sources) or adjust and execute the \"pgwatch3/bootstrap/grafana_datasource.sql\" script on the <code>pgwatch3_grafana</code> DB.</p> </li> <li> <p>Add pgwatch3 predefined dashboards to Grafana</p> <p>This could be done by importing the pgwatch3 dashboard definition JSON-s manually, one by one, from the \"grafana\" folder (\"Import Dashboard\" from the Grafana top menu) or via as small helper script located at /etc/pgwatch3/grafana-dashboards/import_all.sh. The script needs some adjustment for metrics storage type, connect data and file paths.</p> </li> <li> <p>Optionally install also Grafana plugins</p> <p>Currently one pre-configured dashboard (Biggest relations treemap) use an extra plugin - if planning to that dash, then run the following:</p> <pre><code>grafana-cli plugins install savantly-heatmap-panel\n</code></pre> </li> <li> <p>Start discovering the preset dashbaords</p> <p>If the previous step of launching pgwatch3 daemon succeeded and it was more than some minutes ago, one should already see some graphs on dashboards like \"DB overview\" or \"DB overview Unprivileged / Developer mode\" for example.</p> </li> </ol> </li> </ol>"},{"location":"custom_installation/#yaml-based-setup","title":"YAML based setup","text":"<p>From v1.4 one can also deploy the pgwatch3 gatherer daemons more easily in a de-centralized way, by specifying monitoring configuration via YAML files. In that case there is no need for a central Postgres \"config DB\".</p> <p>YAML installation steps</p> <ol> <li>Install pgwatch3 - either from pre-built packages or by compiling     the Go code.</li> <li>Specify hosts you want to monitor and with which metrics /     aggressivness in a YAML file or files, following the example config     located at /etc/pgwatch3/config/instances.yaml or online     here.     Note that you can also use env. variables inside the YAML templates!</li> <li>Bootstrap the metrics storage DB (not needed it using Prometheus     mode).</li> <li>Prepare the \"to-be-monitored\" databases for monitoring by creating     a dedicated login role name as a     <code>minimum &lt;preparing_databases&gt;</code>{.interpreted-text role=\"ref\"}.</li> <li>Run the pgatch2 gatherer specifying the YAML config file (or     folder), and also the folder where metric definitions are located.     Default location: /etc/pgwatch3/metrics.</li> <li>Install and configure Grafana and import the pgwatch3 sample     dashboards to start analyzing the metrics. See above for     instructions.</li> <li>Make sure that there are auto-start SystemD services for all     components in place and optionally set up also backups.</li> </ol> <p>Relevant gatherer parameters / env. vars: <code>--config / PW3_CONFIG</code> and <code>--metrics-folder / PW3_METRICS_FOLDER</code>.</p> <p>For details on individual steps like installing pgwatch3 see the above paragraph.</p>"},{"location":"dashboarding_alerting/","title":"Grafana intro","text":"<p>To display the gathered and stored metrics the pgwatch3 project has decided to rely heavily on the popular Grafana dashboarding solution. This means only though that it's installed in the default Docker images and there's a set of predefined dashboards available to cover most of the metrics gathered via the Preset Configs.</p> <p>This does not mean though that Grafana is in any way tightly coupled with project's other components - quite the opposite actually, one can use any other means / tools to use the metrics data gathered by the pgwatch3 daemon.</p> <p>Currently there are around 30 preset dashboards available for PostgreSQL data sources. Due to that nowadays, if metric gathering volumes are not a problem, we recommend using Postgres storage for most users.</p> <p>Note though that most users will probably want to always adjust the built-in dashboards slightly (colors, roundings, etc) , so that they should be taken only as examples to quickly get started. Also note that in case of changes it's not recommended to change the built-in ones, but use the Save as features - this will allow later to easily update all the dashboards en masse per script, without losing any custom user changes.</p> <p>Links:</p> <p>Built-in dashboards for PostgreSQL (TimescaleDB) storage</p> <p>Screenshots of pgwatch3 default dashboards</p> <p>The online Demo site</p>"},{"location":"dashboarding_alerting/#alerting","title":"Alerting","text":"<p>Alerting is very conveniently also supported by Grafana in a simple point-and-click style - see here for the official documentation. In general all more popular notification services are supported and it's pretty much the easiest way to quickly start with PostgreSQL alerting on a smaller scale. For enterprise usage with hundreds of instances it's might get too \"clicky\" though and there are also some limitations - currently you can set alerts only on Graph panels and there must be no variables used in the query so you cannot use most of the pre-created pgwatch3 graphs, but need to create your own.</p> <p>Nevertheless, alerting via Grafana is s a good option for lighter use cases and there's also a preset dashboard template named \"Alert Template\" from the pgwatch3 project to give you some ideas on what to alert on.</p> <p>Note though that alerting is always a bit of a complex topic - it requires good understanding of PostgreSQL operational metrics and also business criticality background infos, so we don't want to be too opinionated here and it's up to the users to implement.</p>"},{"location":"docker_installation/","title":"Installing using Docker","text":""},{"location":"docker_installation/#simple-setup-steps","title":"Simple setup steps","text":"<p>The simplest real-life pgwatch3 setup should look something like that:</p> <ol> <li> <p>Decide which metrics storage engine you want to use -     cybertec/pgwatch3 uses PostgreSQL. For Prometheus mode (exposing a     port for remote scraping) one should use the slimmer     cybertec/pgwatch3-daemon image which doesn't have any built in     databases.</p> </li> <li> <p>Find the latest pgwatch3 release version by going to the project's     Github Releases page or use the public API with something like     that:</p> <pre><code>curl -so- https://api.github.com/repos/cybertec-postgresql/pgwatch3/releases/latest | jq .tag_name | grep -oE '[0-9\\.]+'\n</code></pre> </li> <li> <p>Pull the image:</p> <pre><code>docker pull cybertec/pgwatch3:X.Y.Z\n</code></pre> </li> <li> <p>Run the Docker image, exposing minimally the Grafana port served on     port 3000 internally. In a relatively secure environment you'd     usually also include the administrative web UI served on port 8080:</p> <pre><code>docker run -d --restart=unless-stopped -p 3000:3000 -p 8080:8080 \\\n--name pw3 cybertec/pgwatch3:X.Y.Z\n</code></pre> <p>Note that we're setting the container to be automatically restarted in case of a reboot/crash - which is highly recommended if not using some container management framework to run pgwatch3.</p> </li> </ol>"},{"location":"docker_installation/#more-future-proof-setup-steps","title":"More future proof setup steps","text":"<p>Although the above simple setup example will do for more temporal setups / troubleshooting sessions, for permanent setups it's highly recommended to create separate volumes for all software components in the container, so that it would be easier to update to newer pgwatch3 Docker images and pull file system based backups and also it might be a good idea to expose all internal ports at least on localhost for possible troubleshooting and making possible to use native backup tools more conveniently for Postgres.</p> <p>Note that for maximum flexibility, security and update simplicity it's best to do a custom setup though - see the next chapter for that.</p> <p>So in short, for plain Docker setups would be best to do something like:</p> <pre><code># let's create volumes for Postgres, Grafana and pgwatch3 marker files / SSL certificates\nfor v in pg  grafana pw3 ; do docker volume create $v ; done\n\n# launch pgwatch3 with fully exposed Grafana and Health-check ports\n# and local Postgres and subnet level Web UI ports\ndocker run -d --restart=unless-stopped --name pw3 \\\n    -p 3000:3000 -p 8081:8081 -p 127.0.0.1:5432:5432 -p 192.168.1.XYZ:8080:8080 \\\n    -v pg:/var/lib/postgresql -v grafana:/var/lib/grafana -v pw3:/pgwatch3/persistent-config \\\n    cybertec/pgwatch3:X.Y.Z\n</code></pre> <p>Note that in non-trusted environments it's a good idea to specify more sensitive ports together with some explicit network interfaces for additional security - by default Docker listens on all network devices!</p> <p>Also note that one can configure many aspects of the software components running inside the container via ENV - for a complete list of all supported Docker environment variables see the ENV_VARIABLES.md file.</p>"},{"location":"docker_installation/#available-docker-images","title":"Available Docker images","text":"<p>Following images are regularly pushed to Docked Hub:</p> <p>cybertec/pgwatch3-demo</p> <p>The original pgwatch3 \u201cbatteries-included\u201d image with PostgreSQL measurements storage. Just insert connect infos to your database via the admin Web UI (or directly into the Config DB) and then turn to the pre-defined Grafana dashboards to analyze DB health and performance.</p> <p>cybertec/pgwatch3</p> <p>A light-weight image containing only the metrics collection daemon / agent, that can be integrated into the monitoring setup over configuration specified either via ENV, mounted YAML files or a PostgreSQL Config DB. See the Component reuse chapter for wiring details.</p>"},{"location":"docker_installation/#building-custom-docker-images","title":"Building custom Docker images","text":"<p>For custom tweaks, more security, specific component versions, etc one could easily build the images themselves, just a Docker installation is needed.</p>"},{"location":"docker_installation/#interacting-with-the-docker-container","title":"Interacting with the Docker container","text":"<ul> <li> <p>If to launch with the <code>PW3_TESTDB=1</code> env. parameter then the     pgwatch3 configuration database running inside Docker is added to     the monitoring, so that you should immediately see some metrics at     least on the Health-check dashboard.</p> </li> <li> <p>To add new databases / instances to monitoring open the     administration Web interface on port 8080 (or some other port, if     re-mapped at launch) and go to the /dbs page. Note that the Web UI     is an optional component, and one can managed monitoring entries     directly in the Postgres Config DB via <code>INSERT</code>-s / <code>UPDATE</code>-s into     <code>\"pgwatch3.monitored_db\"</code> table. Default user/password are again     <code>pgwatch3/pgwatch3admin</code>, database name - <code>pgwatch3</code>. In both     cases note that it can take up to 2min (default main loop time,     changeable via <code>PW3_SERVERS_REFRESH_LOOP_SECONDS</code>) before you see     any metrics for newly inserted databases.</p> </li> <li> <p>One can edit existing or create new Grafana dashboards, change     Grafana global settings, create users, alerts, etc after logging in     as <code>pgwatch3/pgwatch3admin</code> (by default, changeable at launch     time).</p> </li> <li> <p>Metrics and their intervals that are to be gathered can be     customized for every database separately via a custom JSON config     field or more conveniently by using Preset Configs, like     \"minimal\", \"basic\" or \"exhaustive\" (<code>monitored_db.preset_config</code>     table), where the name should already hint at the amount of metrics     gathered. For privileged users the \"exhaustive\" preset is a good     starting point, and \"unprivileged\" for simple developer accounts.</p> </li> <li> <p>To add a new metrics yourself (which are simple SQL queries     returning any values and a timestamp) head to     http://127.0.0.1:8080/metrics. The queries should always include a     <code>\"epoch_ns\"</code> column and <code>\"tag\\_\"</code> prefix can be used for columns     that should be quickly searchable/groupable, and thus will be     indexed with the PostgreSQL metric stores. See to the bottom of the     \"metrics\" page for more explanations or the documentation chapter     on metrics.</p> </li> <li> <p>For a quickstart on dashboarding, a list of available metrics     together with some instructions are presented on the     \"Documentation\" dashboard.</p> </li> <li> <p>Some built-in metrics like <code>\"cpu_load\"</code> and others, that gather     privileged or OS statistics, require installing helper functions     (looking like     that,     so it might be normal to see some blank panels or fetching errors in     the logs. On how to prepare databases for monitoring see the     Monitoring preparations chapter.</p> </li> <li> <p>For effective graphing you want to familiarize yourself with the     query language of the database system that was selected for metrics     storage. Some tips to get going:</p> <ul> <li>For PostgreSQL/TimescaleDB - some knowledge of Window     functions     is a must if looking at longer time periods of data as the     statistics could have been reset in the mean time by user     request or the server might have crashed, so that simple     <code>max() - min()</code> aggregates on cumulative counters (most data     provided by Postgres is cumulative) would lie.</li> </ul> </li> <li> <p>For possible troubleshooting needs, logs of the components running     inside Docker are by default (if not disabled on container launch)     visible under:     http://127.0.0.1:8080/logs/%5Bpgwatch3%7Cpostgres%7Cwebui%7Cgrafana.     It's of course also possible to log into the container and look at     log files directly - they're situated under     <code>/var/logs/supervisor/</code>.</p> <p>FYI - <code>docker logs ...</code> command is not really useful after a successful container startup in pgwatch3 case.</p> </li> </ul>"},{"location":"docker_installation/#ports-used","title":"Ports used","text":"<ul> <li>5432 - Postgres configuration or metrics storage DB (when using the     cybertec/pgwatch3 image)</li> <li>8080 - Management Web UI (monitored hosts, metrics, metrics     configurations)</li> <li>8081 - Gatherer healthcheck / statistics on number of gathered     metrics (JSON).</li> <li>3000 - Grafana dashboarding</li> </ul>"},{"location":"docker_installation/#docker-compose","title":"Docker Compose","text":"<p>As mentioned in the Components chapter, remember that the pre-built Docker images are just one example how your monitoring setup around the pgwatch3 metrics collector could be organized. For another example how various components (as Docker images here) can work together, see a Docker Compose example with loosely coupled components here.</p>"},{"location":"features/","title":"List of main features","text":"<ul> <li>Non-invasive setup on PostgreSQL side - no extensions nor superuser     rights are required for the base functionality so that even     unprivileged users like developers can get a good overview of     database activities without any hassle</li> <li>Lots of preset metric configurations covering all performance     critical PostgreSQL internal Statistics Collector data</li> <li>Intuitive metrics presentation using a set of predefined dashboards     for the very popular Grafana dashboarding engine with optional     alerting support</li> <li>Easy extensibility of metrics which are defined in pure SQL, thus     they could also be from the business domain</li> <li>Many metric data storage options - PostgreSQL, PostgreSQL with the     compression enabled TimescaleDB extension, or Prometheus scraping</li> <li>Multiple deployment options - PostgreSQL configuration DB, YAML or     ENV configuration, supporting both \"push\" and \"pull\" models</li> <li>Possible to monitoring all, single or a subset (list or regex) of     databases of a PostgreSQL instance</li> <li>Global or per DB configuration of metrics and metric fetching     intervals and optionally also times / days</li> <li>Kubernetes/OpenShift ready with sample templates and a Helm chart</li> <li>PgBouncer, Pgpool2, AWS RDS and Patroni support with automatic     member discovery</li> <li>Internal health-check API (port 8081 by default) to monitor metrics     gathering status / progress remotely</li> <li>Built-in security with SSL connections support for all components     and passwords encryption for connect strings</li> <li>Very low resource requirements for the collector even when     monitoring hundreds of instances</li> <li>Capabilities to go beyond PostgreSQL metrics gathering with built-in     log parsing for error detection and OS level metrics collection via     PL/Python \"helper\" stored procedures</li> <li>A Ping mode to test connectivity to all databases under monitoring</li> </ul>"},{"location":"installation_options/","title":"Installation options","text":"<p>Besides freedom of choosing from a set of metric storage options one can also choose how they're going to retrieve metrics from databases - in a \"pull\" or \"push\" way and how is the monitoring configuration (connect strings, metric sets and intervals) going to be stored.</p>"},{"location":"installation_options/#config-db-based-operation","title":"Config DB based operation","text":"<p>This is the original central pull mode depicted on the architecture diagram.  It requires a small schema to be rolled out on any Postgres database accessible to the metrics gathering daemon, which will hold the connect strings, metric definition SQL-s and preset configurations and some other more minor attributes. For rollout details see the custom installation chapter.</p> <p>The default Docker images use this approach.</p>"},{"location":"installation_options/#file-based-operation","title":"File based operation","text":"<p>From v1.4.0 one can deploy the gatherer daemon(s) decentrally with hosts to be monitored defined in simple YAML files. In that case there is no need for the central Postgres \"config DB\". See the sample instances.yaml config file for an example. Note that in this mode you also need to point out the path to metric definition SQL files when starting the gatherer. Also note that the configuration system also supports multiple YAML files in a folder so that you could easily programmatically manage things via Ansible for example and you can also use Env. vars in sideYAML files.</p> <p>Relevant Gatherer env. vars / flags: <code>--config, --metrics-folder</code> or <code>PW3_CONFIG / PW3_METRICS_FOLDER</code>.</p>"},{"location":"installation_options/#prometheus-mode","title":"Prometheus mode","text":"<p>In v1.6.0 was added support for Prometheus - being one of the most popular modern metrics gathering / alerting solutions. When the <code>--datastore / PW3_DATASTORE</code> parameter is set to prometheus then the pgwatch3 metrics collector doesn't do any normal interval-based fetching but listens on port 9187 (changeable) for scrape requests configured and performed on Prometheus side. Returned metrics belong to the \"pgwatch3\" namespace (a prefix basically) which is changeable via the <code>--prometheus-namespace</code> flag if needed.</p> <p>Also important to note - in this mode the pgwatch3 agent should not be run centrally but on all individual DB hosts. While technically possible though to run centrally, it would counter the core idea of Prometheus and would make scrapes also longer and risk getting timeouts as all DBs are scanned sequentially for metrics.</p> <p>FYI -- the functionality has some overlap with the existing postgres_exporter project, but also provides more flexibility in metrics configuration and all config changes are applied \"online\".</p> <p>Also note that Prometheus can only store numerical metric data values - so metrics output for example PostgreSQL storage and Prometheus are not 100% compatile. Due to that there's also a separate \"preset config\" named \"prometheus\".</p>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>A basic Helm chart is available for installing pgwatch3 to a Kubernetes cluster. The corresponding setup can be found in [./openshift_k8s/helm-chart]{.title-ref}, whereas installation is done via the following commands:</p> <pre><code>cd openshift_k8s\nhelm install -f chart-values.yml pgwatch3 ./helm-chart\n</code></pre> <p>Please have a look at openshift_k8s/helm-chart/values.yaml to get additional information of configurable options.</p>"},{"location":"long_term_installations/","title":"Long term installations","text":"<p>For long term pgwatch3 setups the main challenge is to keep the software up-to-date to guarantee stable operation and also to make sure that all DB-s are under monitoring.</p>"},{"location":"long_term_installations/#keeping-inventory-in-sync","title":"Keeping inventory in sync","text":"<p>Adding new DBs to monitoring and removing those shut down, can become a problem if teams are big, databases are many, and it's done per hand (common for on-premise, non-orchestrated deployments). To combat that, the most typical approach would be to write some script or Cronjob that parses the company's internal inventory database, files or endpoints and translate changes to according CRUD operations on the pgwatch3.monitored_db table directly.</p> <p>One could also use the Web UI page (pseudo) API for that purpose, if the optional Web UI component has been deployed. See here for an usage example - but direct database access is of course more flexible.</p> <p>If pgwatch3 configuration is kept in YAML files, it should be also relatively easy to automate the maintenance as the configuration can be organized so that one file represent a single monitoring entry, i.e. the --config parameter can also refer to a folder of YAML files.</p>"},{"location":"long_term_installations/#updating-the-pgwatch3-collector","title":"Updating the pgwatch3 collector","text":"<p>The pgwatch3 metrics gathering daemon is the core component of the solution alas the most critical one. So it's definitely recommended to update it at least once per year or minimally when some freshly released Postgres major version instances are added to monitoring. New Postgres versions don't necessary mean that something will break, but you'll be missing some newly added metrics, plus the occasional optimizations. See the ref:`upgrading chapter \\&lt;upgrading&gt;` for details, but basically the process is very similar to initial installation as the collector doesn't have any state on its own - it's just on binary program.</p>"},{"location":"long_term_installations/#metrics-maintenance","title":"Metrics maintenance","text":"<p>Metric definition SQL-s are regularly corrected as suggestions / improvements come in and also new ones are added to cover latest Postgres versions, so would make sense to refresh them 1-2x per year.</p> <p>If using a YAML based config, just installing newer pre-built RPM / DEB packages will do the trick automatically (built-in metrics at /etc/pgwatch3/metrics will be refreshed) but for Config DB based setups you'd need to follow a simple process described <code>here &lt;updating_metrics&gt;</code>{.interpreted-text role=\"ref\"}.</p>"},{"location":"long_term_installations/#dashboard-maintenance-dashboard_maintenance","title":"Dashboard maintenance {#dashboard_maintenance}","text":"<p>Same as with metrics, also the built-in Grafana dashboards are being actively updates, so would make sense to refresh them occasionally also. The bulk delete / import scripts can be found here or you could also manually just re-import some dashboards of interest from JSON files in [/etc/pgwatch3/grafana-dashboards]{.title-ref} folder or from Github.</p> <p>The delete_all_old_pw3_dashes.sh script deletes all pgwatch3 built-in dashboards so you should take some extra care when you've changed them. In general it's a good idea not to modify the preset dashboards too much, but rate use the \"Save As...\" button and rename the dashboards to something else.</p> <p>FYI - notable new dashboards are usually listed also in release notes and most dashboards also have a sample screenshots available.</p>"},{"location":"long_term_installations/#storage-monitoring","title":"Storage monitoring","text":"<p>In addition to all that you should at least initially periodically monitor the metrics DB size...as it can grow quite a lot (especially when using Postgres for storage) when the monitored databases have hundreds of tables / indexes and if a lot of unique SQL-s are used and pg_stat_statements monitoring is enabled. If the storage grows too fast, one can increase the metric intervals (especially for \"table_stats\", \"index_stats\" and \"stat_statements\") or decrease the data retention periods via --pg-retention-days or --iretentiondays params.</p>"},{"location":"metric_definitions/","title":"Metric definitions","text":"<p>Metrics are named SQL queries that return a timestamp and pretty much anything else you find useful. Most metrics have many different query text versions for different target PostgreSQL versions, also optionally taking into account primary / replica state and as of v1.8 also versions of installed extensions.</p> <pre><code>-- a sample metric\nSELECT\n  (extract(epoch from now()) * 1e9)::int8 as epoch_ns,\n  extract(epoch from (now() - pg_postmaster_start_time()))::int8 as postmaster_uptime_s,\n  case when pg_is_in_recovery() then 1 else 0 end as in_recovery_int;\n</code></pre> <p>Correct version of the metric definition will be chosen automatically by regularly connecting to the target database and checking the Postgres version, recovery state, and if the monitoring user is a superuser or not. For superusers some metrics have alternate SQL definitions (as of v1.6.2) so that no \"helpers\" are needed for Postgres-native Stats Collector infos. Using superuser accounts for remote monitoring is of course not really recommended.</p> <p>There's a good set of pre-defined metrics &amp; metric configs provided by the pgwatch3 project to cover all typical needs, but when monitoring hundreds of hosts you'd typically want to develop some custom Preset Configs or at least adjust the metric fetching intervals according to your monitoring goals.</p> <p>Some things to note about the built-in metrics:</p> <ul> <li>Only a half of them are included in the Preset configs and are     ready for direct usage. The rest need some extra extensions or     privileges, OS level tool installations etc. To see what's possible     just browse the sample     metrics.</li> <li>Some builtin metrics are marked to be only executed when server is a     primary or conversely, a standby. The flags can be inspected / set     on the Web UI Metrics tab or in YAML mode by suffixing the metric     definition with \"standby\" or \"master\". Note that starting from     v1.8.1 it's also possible to specify completely alternative     monitoring configurations, i.e. metric-interval pairs, for the     \"standby\" (recovery) state - by default the same set of metrics     are used for both states.</li> <li> <p>There are a couple of special preset metrics that have some     non-standard behaviour attached to them:</p> <p>change_events</p> <p>:   The \"change_events\" built-in metric, tracking DDL &amp; config     changes, uses internally some other \"*_hashes\" metrics which     are not meant to be used on their own. Such metrics are     described also accordingly on the Web UI /metrics page and they     should not be removed.</p> <p>recommendations</p> <p>:   When enabled (i.e. interval &gt; 0), this metric will find all     other metrics starting with \"reco_*\" and execute those     queries. The purpose of the metric is to spot some performance,     security and other \"best practices\" violations. Users can add     new \"reco_*\" queries freely.</p> <p>server_log_event_counts</p> <p>:   This enables Postgres server log \"tailing\" for errors. Can't     be used for \"pull\" setups though unless the DB logs are     somehow mounted / copied over, as real file access is needed.     See the <code>Log parsing &lt;log_parsing&gt;</code>{.interpreted-text     role=\"ref\"} chapter for details.</p> <p>instance_up</p> <p>:   For normal metrics there will be no data rows stored if the DB     is not reachable, but for this one there will be a 0 stored for     the \"is_up\" column that under normal operations would always     be 1. This metric can be used to calculate some \"uptime\" SLA     indicator for example.</p> </li> </ul>"},{"location":"metric_definitions/#defining-custom-metrics","title":"Defining custom metrics","text":"<p>For defining metrics definitions you should adhere to a couple of basic concepts:</p> <ul> <li>Every metric query should have an \"epoch_ns\" (nanoseconds since     epoch column to record the metrics reading time. If the column is     not there, things will still work but server timestamp of the     metrics gathering daemon will be used, some a small loss (assuming     intra-datacenter monitoring with little lag) of precision occurs.</li> <li>Queries can only return text, integer, boolean or floating point     (a.k.a. double precision) Postgres data types. Note that columns     with NULL values are not stored at all in the data layer as it's a     bit bothersome to work with NULLs!</li> <li>Column names should be descriptive enough so that they're     self-explanatory, but not over long as it costs also storage</li> <li>Metric queries should execute fast - at least below the selected     Statement timeout (default 5s)</li> <li>Columns can be optionally \"tagged\" by prefixing them with     \"tag_\". By doing this, the column data will be indexed by the     Postgres giving following advantages:<ul> <li>Sophisticated auto-discovery support for indexed keys/values,     when building charts with Grafana.</li> <li>Faster queries for queries on those columns.</li> </ul> </li> <li>All fetched metric rows can also be \"prettyfied\" with any custom     static key-value data, per host. To enable use the \"Custom tags\"     Web UI field for the monitored DB entry or \"custom_tags\" YAML     field. Note that this works per host and applies to all metrics.</li> <li>For Prometheus the numerical columns are by default mapped to a     Value Type of \"Counter\" (as most Statistics Collector columns are     cumulative), but when this is not the case and the column is a     \"Gauge\" then according column attributes should be declared. See     below section on column attributes for details.</li> <li>For Prometheus all text fields will be turned into tags / labels as     only floats can be stored!</li> </ul> <p>Adding and using a custom metric:</p> <p>For Config DB based setups:</p> <ol> <li>Go to the Web UI \"Metric definitions\" page and scroll to the     bottom.</li> <li>Fill the template - pick a name for your metric, select minimum     supported PostgreSQL version and insert the query text and any     extra attributes if any (see below for options). Hit the \"New\"     button to store.</li> <li>Activate the newly added metric by including it in some existing     Preset Config (listed on top of the page) or add it directly in     JSON form, together with an interval, into the \"Custom metrics     config\" filed on the \"DBs\" page.</li> </ol> <p>For YAML based setups:</p> <ol> <li>Create a new folder for the metric under     \"/etc/pgwatch3/metrics\". The folder name will be the metric's     name, so choose wisely.</li> <li>Create a new subfolder for each \"minimally supported Postgres     version* and insert the metric's SQL definition into a file     named \"metric.sql\". Note the \"minimally supported\" part - i.e.     if your query will work from version v9.0 to v13 then you only     need one folder called \"9.0\". If there was a breaking change in     the internal catalogs at v9.6 so that the query stopped working,     you need a new folder named \"9.6\" that will be used for all     versions above v9.5.</li> <li>Activate the newly added metric by including it in some existing     Preset Config (/etc/pgwatch3/metrics/preset-configs.yaml) or add     it directly to the YAML config \"custom_metrics\" section.</li> </ol>"},{"location":"metric_definitions/#metric-attributes","title":"Metric attributes","text":"<p>Since v1.7 behaviour of plain metrics can be extended with a set of attributes that will modify the gathering in some way. The attributes are stored in YAML files called metric_attrs.yaml\" in a metrics root directory or in themetric_attribute* Config DB table.</p> <p>Currently supported attributes:</p> <p>is_instance_level</p> <p>:   Enables caching, i.e. sharing of metric data between various     databases of a single instance to reduce load on the monitored     server.</p> <p>statement_timeout_seconds</p> <p>:   Enables to override the default 'per monitored DB' statement     timeouts on metric level.</p> <p>metric_storage_name</p> <p>:   Enables dynamic \"renaming\" of metrics at storage level, i.e.     declaring almost similar metrics with different names but the data     will be stored under one metric. Currently used (for out-of-the box     metrics) only for the 'stat_statements_no_query_text' metric, to     not to store actual query texts from the \"pg_stat_statements\"     extension for more security sensitive instances.</p> <p>extension_version_based_overrides</p> <p>:   Enables to \"switch out\" the query text from some other metric     based on some specific extension version. See 'reco_add_index' for     an example definition.</p> <p>disabled_days</p> <p>:   Enables to \"pause\" metric gathering on specified days. See     metric_attrs.yaml for \"wal\" for an example.</p> <p>disabled_times</p> <p>:   Enables to \"pause\" metric gathering on specified time intervals.     e.g. \"09:00-17:00\" for business hours. Note that if time zone is     not specified the server time of the gather daemon is used.     disabled_days / disabled_times can also be defined both on metric     and host (host_attrs) level.</p> <p>For a sample definition see here.</p>"},{"location":"metric_definitions/#column-attributes","title":"Column attributes","text":"<p>Besides the _tag column prefix modifier, it's also possible to modify the output of certain columns via a few attributes. It's only relevant for Prometheus output though currently, to set the correct data types in the output description, which is generally considered a nice-to-have thing anyways. For YAML based setups this means adding a \"column_attrs.yaml\" file in the metric's top folder and for Config DB based setup an according \"column_attrs\" JSON column should be filled via the Web UI.</p> <p>Supported column attributes:</p> <p>prometheus_gauge_columns</p> <p>:   Describe the mentioned output columns as of TYPE gauge, i.e. the     value can change any time in any direction. Default TYPE for     pgwatch3 is counter.</p>"},{"location":"metric_definitions/#adding-metric-fetching-helpers","title":"Adding metric fetching helpers","text":"<p>As mentioned in <code>Helper Functions &lt;helper_functions&gt;</code>{.interpreted-text role=\"ref\"} section, Postgres knows very little about the Operating System that it's running on, so in some (most) cases it might be advantageous to also monitor some basic OS statistics together with the PostgreSQL ones, to get a better head start when troubleshooting performance problems. But as setup of such OS tools and linking the gathered data is not always trivial, pgwatch3 has a system of helpers for fetching such data.</p> <p>One can invent and install such helpers on the monitored databases freely to expose any information needed (backup status etc) via Python, or any other PL-language supported by Postgres, and then add according metrics similarly to any normal Postgres-native metrics.</p>"},{"location":"preparing_databases/","title":"Effects of monitoring","text":"<ul> <li>Although the \"Observer effect\" applies also for pgwatch3, no     noticeable impact for the monitored DB is expected when using     Preset configs settings, and given that there is some normal load     on the server anyways and the DB doesn't have thousands of tables.     For some metrics though can happen that the metric reading query     (notably \"stat_statements\" and \"table_stats\") takes some tens of     milliseconds, which might be more than an average application query.</li> <li>At any time maximally 2 metric fetching queries can run in parallel     on any monitored DBs. This can be changed by recompiling     (MAX_PG_CONNECTIONS_PER_MONITORED_DB variable) the gatherer.</li> <li>Default Postgres statement     timeout     is 5s for entries inserted via the Web UI / database directly.</li> </ul>"},{"location":"preparing_databases/#basic-preparations","title":"Basic preparations","text":"<p>As a base requirement you'll need a login user (non-superuser suggested) for connecting to your server and fetching metrics.</p> <p>Though theoretically you can use any username you like, but if not using \"pgwatch3\" you need to adjust the \"helper\" creation SQL scripts (see below for explanation) accordingly, as in those by default the \"pgwatch3\" will be granted execute privileges.</p> <pre><code>CREATE ROLE pgwatch3 WITH LOGIN PASSWORD 'secret';\n-- For critical databases it might make sense to ensure that the user account\n-- used for monitoring can only open a limited number of connections\n-- (there are according checks in code, but multiple instances might be launched)\nALTER ROLE pgwatch3 CONNECTION LIMIT 3;\nGRANT pg_monitor TO pgwatch3;   // v10+\nGRANT CONNECT ON DATABASE mydb TO pgwatch3;\nGRANT USAGE ON SCHEMA public TO pgwatch3; -- pgwatch doesn't necessarily require using the public schema though!\nGRANT EXECUTE ON FUNCTION pg_stat_file(text) to pgwatch3; -- needed by the wal_size metric\n</code></pre> <p>For most monitored databases it's extremely beneficial (to troubleshooting performance issues) to also activate the pg_stat_statements extension which will give us exact \"per query\" performance aggregates and also enables to calculate how many queries are executed per second for example. In pgwatch3 context it powers the \"Stat statements Top\" dashboard and many other panels of other dashboards. For additional troubleshooting benefits also the track_io_timing setting should be enabled.</p> <ol> <li> <p>Make sure the Postgres contrib package is installed (should be     installed automatically together with the Postgres server package on     Debian based systems).</p> <ul> <li>On RedHat / Centos: <code>yum install -y postgresqlXY-contrib</code></li> <li>On Debian / Ubuntu: <code>apt install postgresql-contrib</code></li> </ul> </li> <li> <p>Add pg_stat_statements to your server config (postgresql.conf) and     restart the server.</p> <pre><code>shared_preload_libraries = 'pg_stat_statements'\ntrack_io_timing = on\n</code></pre> </li> <li> <p>After restarting activate the extension in the monitored DB. Assumes     Postgres superuser.</p> <pre><code>psql -c \"CREATE EXTENSION IF NOT EXISTS pg_stat_statements\"\n</code></pre> </li> </ol>"},{"location":"preparing_databases/#rolling-out-helper-functions-helper_functions","title":"Rolling out helper functions {#helper_functions}","text":"<p>Helper functions in pgwatch3 context are standard Postgres stored procedures, running under SECURITY DEFINER privileges. Via such wrapper functions one can do controlled privilege escalation - i.e. to give access to protected Postgres metrics (like active session details, \"per query\" statistics) or even OS-level metrics, to normal unprivileged users, like the pgwatch3 monitoring role.</p> <p>If using a superuser login (recommended only for local \"push\" setups) you have full access to all Postgres metrics and would need helpers only for OS remote statistics. For local (push) setups as of pgwatch3 version 1.8.4 the most typical OS metrics are covered by the \"--direct-os-stats\" flag, explained below.</p> <p>For unprivileged monitoring users it is highly recommended to take these additional steps on the \"to be monitored\" database to get maximum value out of pgwatch3 in the long run. Without these additional steps, you lose though about 10-15% of built-in metrics, which might not be too tragical nevertheless. For that use case there's also a preset config named \"unprivileged\".</p> <p>When monitoring v10+ servers then the built-in pg_monitor system role is recommended for the monitoring user, which almost substitutes superuser privileges for monitoring purposes in a safe way.</p> <p>Rolling out common helpers</p> <p>For completely unprivileged monitoring users the following helpers are recommended to make good use of the default \"exhaustive\" Preset Config:</p> <pre><code>export PGUSER=superuser\npsql -f /etc/pgwatch3/metrics/00_helpers/get_stat_activity/$pgver/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_stat_replication/$pgver/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_wal_size/$pgver/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_stat_statements/$pgver/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_sequences/$pgver/metric.sql mydb\n</code></pre> <p>Note that there might not be an exact Postgres version match for helper definitions - then replace \\$pgver with the previous available version number below your server's Postgres version number.</p> <p>Also note that as of v1.8.1 some helpers definition SQL-s scripts (like for \"get_stat_statements\") will inspect also the \"search_path\" and by default will not install into schemas that have PUBLIC CREATE privileges, like the \"public\" schema by default has!</p> <p>Also when rolling out helpers make sure the [search_path]{.title-ref} is at defaults or set so that it's also accessible for the monitoring role as currently neither helpers nor metric definition SQL-s don't assume any particualar schema and depend on the [search_path]{.title-ref} including everything needed.</p> <p>For more detailed statistics (OS monitoring, table bloat, WAL size, etc) it is recommended to install also all other helpers found from the [/etc/pgwatch3/metrics/00_helpers]{.title-ref} folder or do it automatically by using the rollout_helper.py script found in the 00_helpers folder.</p> <p>As of v1.6.0 though helpers are not needed for Postgres-native metrics (e.g. WAL size) if a privileged user (superuser or pg_monitor GRANT) is used, as pgwatch3 now supports having 2 SQL definitions for each metric - \"normal / unprivileged\" and \"privileged\" / \"superuser\". In the file system /etc/pgwatch3/metrics such \"privileged\" access definitions will have a \"_su\" added to the file name.</p>"},{"location":"preparing_databases/#automatic-rollout-of-helpers","title":"Automatic rollout of helpers","text":"<p>pgwatch3 can roll out helpers also automatically on the monitored DB. This requires superuser privileges and a configuration attribute for the monitored DB. In YAML config mode it's called is_superuser, in Config DB md_is_superuser, in the Web UI one can tick the \"Auto-create helpers\" checkbox.</p> <p>After the automatic rollout it's still generally recommended to remove the superuser privileges from the monitoring role, which now should have GRANT-s to all automatically created helper functions. Note though that all created helpers will not be immediately usable as some are for special purposes and need additional dependencies.</p> <p>A hint: if it can be foreseen that a lot of databases will be created on some instance (generally not a good idea though) it might be a good idea to roll out the helpers directly in the template1 database - so that all newly created databases will get them automatically.</p>"},{"location":"preparing_databases/#plpython-helpers","title":"PL/Python helpers","text":"<p>PostgreSQL in general is implemented in such a way that it does not know too much about the operation system that it is running on. This is a good thing for portability but can be somewhat limiting for monitoring, especially when there is no system monitoring framework in place or the data is not conveniently accessible together with metrics gathered from Postgres. To overcome this problem, users can also choose to install helpers extracting OS metrics like CPU, RAM usage, etc so that this data is stored together with Postgres-native metrics for easier graphing / correlation / alerting. This also enable to be totally independent of any System Monitoring tools like Zabbix, etc, with the downside that everything is gathered over Postgres connections so that when Postgres is down no OS metrics can be gathered also. Since v1.8.4 though the latter problem can be reduced for local \"push\" based setups via the \"--direct-os-stats\" option plus according metrics configuration (e.g. the \"full\" preset).</p> <p>Note though that PL/Python is usually disabled by DB-as-a-service providers like AWS RDS for security reasons.</p> <pre><code># first install the Python bindings for Postgres\napt install postgresql-plpython3-XY\n# yum install postgresqlXY-plpython3\n\npsql -c \"CREATE EXTENSION plpython3u\"\npsql -f /etc/pgwatch3/metrics/00_helpers/get_load_average/9.1/metric.sql mydb\n\n# psutil helpers are only needed when full set of common OS metrics is wanted\napt install python3-psutil\npsql -f /etc/pgwatch3/metrics/00_helpers/get_psutil_cpu/9.1/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_psutil_mem/9.1/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_psutil_disk/9.1/metric.sql mydb\npsql -f /etc/pgwatch3/metrics/00_helpers/get_psutil_disk_io_total/9.1/metric.sql mydb\n</code></pre> <p>Note that we're assuming here that we're on a modern Linux system with Python 3 as default. For older systems Python 3 might not be an option though, so you need to change plpython3u to plpythonu and also do the same replace inside the code of the actual helper functions! Here the rollout_helper.py script with it's <code>--python2</code> flag can be helpful again.</p>"},{"location":"preparing_databases/#notice-on-using-metric-fetching-helpers","title":"Notice on using metric fetching helpers","text":"<ul> <li>Starting from Postgres v10 helpers are mostly not needed (only for     PL/Python ones getting OS statistics) - there are available some     special monitoring roles like \"pg_monitor\", that are exactly meant     to be used for such cases where we want to give access to all     Statistics Collector views without any other \"superuser     behaviour\". See     here     for documentation on such special system roles. Note that currently     most out-of-the-box metrics first rely on the helpers as v10 is     relatively new still, and only when fetching fails, direct access     with the \"Privileged SQL\" is tried.</li> <li>For gathering OS statistics (CPU, IO, disk) there are helpers and     metrics provided, based on the \"psutil\" Python package...but from     user reports seems the package behaviour differentiates slightly     based on the Linux distro / Kernel version used, so small     adjustments might be needed there (e.g. to remove a non-existent     column). Minimum usable Kernel version required is 3.3. Also note     that SQL helpers functions are currently defined for Python 3, so     for older Python 2 you need to change the <code>LANGUAGE plpython3u</code>     part.</li> <li>When running the gatherer locally, i.e. having a \"push\" based     configuration, the metric fetching helpers are not mostly not needed     as superuser can be used in a safe way and starting from v1.8.4 one     can also enable the --direct-os-stats parameter to signal that     we can fetch the data for the default \"psutil*\" metrics     directly from OS counters. If direct OS fetching fails though, the     fallback is still to try via PL/Python wrappers.</li> <li>In rare cases when some \"helpers\" have been installed, and when     doing a binary PostgreSQL upgrade at some later point in time via     [pg_upgrade]{.title-ref}, this could result in error messages     thrown. Then just drop those failing helpers on the \"to be     upgraded\" cluster and re-create them after the upgrade process.</li> </ul>"},{"location":"preparing_databases/#running-with-developer-credentials","title":"Running with developer credentials","text":"<p>As mentioned above, helper / wrapper functions are not strictly needed, they just provide a bit more information for unprivileged users - thus for developers with no means to install any wrappers as superuser, it's also possible to benefit from pgwatch3 - for such use cases e.g. the \"unprivileged\" preset metrics profile and the according \"DB overview Unprivileged / Developer\" dashboard are a good starting point as it only assumes existence of [pg_stat_statements]{.title-ref} (which should be available by all cloud providers).</p>"},{"location":"preparing_databases/#different-db-types-explained-db_types","title":"Different DB types explained {#db_types}","text":"<p>When adding a new \"to be monitored\" entry a DB type needs to be selected. Following types are available:</p> <p>postgres</p> <p>:   Monitor a single database on a single Postgres instance. When using     the Web UI and the \"DB name\" field is left empty, there's as a     one time operation where all non-template DB names are fetched,     prefixed with \"Unique name\" field value and added to monitoring     (if not already monitored). Internally monitoring always happens     \"per DB\" not \"per cluster\" though.</p> <p>postgres-continuous-discovery</p> <p>:   Monitor a whole (or subset of DB-s) of Postgres cluster / instance.     Host information without a DB name needs to be specified and then     the pgwatch3 daemon will periodically scan the cluster and add any     found and not yet monitored DBs to monitoring. In this mode it's     also possible to specify regular expressions to include/exclude some     database names.</p> <p>pgbouncer</p> <p>:   Use to track metrics from PgBouncer's \"SHOW STATS\" command. In     place of the Postgres \"DB name\" the name of the PgBouncer \"pool\"     to be monitored must be inserted.</p> <p>pgpool</p> <p>:   Use to track joint metrics from Pgpool2's SHOW POOL_NODES and     POOL_PROCESSES commands. Pgpool2 from version 3.0 is supported.</p> <p>patroni</p> <p>:   Patroni is a HA / cluster manager for Postgres that relies on a DCS     (Distributed Consensus Store) to store it's state. Typically in     such a setup the nodes come and go and also it should not matter who     is currently the master. To make it easier to monitor such dynamic     constellations pgwatch3 supports reading of cluster node info from     all supported DCS-s (etcd, Zookeeper, Consul), but currently only     for simpler cases with no security applied (which is actually the     common case in a trusted environment).</p> <p>patroni-continuous-discovery</p> <p>:   As normal patroni DB type but all DB-s (or only those matching the     regex if any provided) are monitored.</p> <p>patroni-namespace-discovery</p> <p>:   Similar to patroni-continuous-discovery but all Patroni scopes     (clusters) of an ETCD namespace are automatically monitored.     Optionally regexes on database names still apply if provided.</p> <p>All \"continuous\" modes expect access to \"template1\" or \"postgres\" databasess of the specified cluster to determine the database names residing there.</p>"},{"location":"project_background/","title":"Project background","text":"<p>The pgwatch3 project got started back in 2016 and released in 2017 initially for internal monitoring needs at Cybertec as all the Open Source PostgreSQL monitoring tools at the time had various limitations like being too slow and invasive to set up or providing a fixed set of visuals and metrics.</p> <p>For more background on the project motivations and design goals see the original series of blogposts announcing the project and the following feature updates released approximately twice per year.</p> <p>Cybertec also provides commercial 9-to-5 and 24/7 support for pgwatch3.</p> <ul> <li>Project     announcement</li> <li>Implementation     details</li> <li>Feature pack     1</li> <li>Feature pack     2</li> <li>Feature pack     3</li> <li>Feature pack     4</li> <li>Feature pack     5</li> <li>Feature pack     6</li> <li>Feature pack     7</li> </ul>"},{"location":"project_background/#project-feedback","title":"Project feedback","text":"<p>For feature requests or troubleshooting assistance please open an issue on project's Github page.</p>"},{"location":"security/","title":"General security information","text":"<p>Security can be tightened for most pgwatch3 components quite granularly, but the default values for the Docker image don't focus on security though but rather on being quickly usable for ad-hoc performance troubleshooting, which is where the roots of pgwatch3 lie.</p> <p>Some points on security:</p> <ul> <li> <p>Starting from v1.3.0 there's a non-root Docker version available     (suitable for OpenShift)</p> </li> <li> <p>The administrative Web UI doesn't have by default any security.     Configurable via env. variables.</p> </li> <li> <p>Viewing Grafana dashboards by default doesn't require login.     Editing needs a password. Configurable via env. variables.</p> </li> <li> <p>Dashboards based on the \"stat_statements\" metric (Stat Statement     Overview / Top) expose actual queries.</p> <p>They should be \"mostly\" stripped of details though and replaced by placeholders by Postgres, but if no risks can be taken such dashboards (or at least according panels) should be deleted. Or as an alternative the \"stat_statements_no_query_text\" and \"pg_stat_statements_calls\" metrics could be used, which don't store query texts in the first place.</p> </li> <li> <p>Safe certificate connections to Postgres are supported as of v1.5.0</p> <p>According sslmode (verify-ca, verify-full) and cert file paths need to be specified then on Web UI \"/dbs\" page or in the YAML config.</p> </li> <li> <p>Encryption / decryption of connection string passwords stored in the     config DB or in YAML config files</p> <p>By default passwords are stored in plaintext but as of v1.5 it's possible to use an encryption passphrase, or a file with the passphrase in it, via --aes-gcm-keyphrase / --aes-gcm-keyphrase-file or PW3_AES_GCM_KEYPHRASE / PW3_AES_GCM_KEYPHRASE_FILE parameters. If using the Web UI to store connection info, the same encryption key needs to be specified for both the Web UI and the gatherer. If using YAML configs then encrypted passwords can be generated using the --aes-gcm-password-to-encrypt flag for embedding in YAML.</p> <p>Note that although pgwatch3 can handle password security, in many cases it's better to still use the standard LibPQ .pgpass file to store passwords.</p> </li> </ul>"},{"location":"security/#launching-a-more-secure-docker-container","title":"Launching a more secure Docker container","text":"<p>Some common sense security is built into default Docker images for all components but not actived by default. A sample command to launch pgwatch3 with following security \"checkpoints\" enabled:</p> <ol> <li>HTTPS for both Grafana and the Web UI with self-signed certificates</li> <li>No anonymous viewing of graphs in Grafana</li> <li>Custom user / password for the Grafana \"admin\" account</li> <li>No anonymous access / editing over the admin Web UI</li> <li>No viewing of internal logs of components running inside Docker</li> <li>Password encryption for connect strings stored in the Config DB</li> </ol> <pre><code>docker run --name pw3 -d --restart=unless-stopped \\\n  -p 3000:3000 -p 8080:8080 \\\n  -e PW3_GRAFANASSL=1 -e PW3_WEBSSL=1 \\\n  -e PW3_GRAFANANOANONYMOUS=1 -e PW3_GRAFANAUSER=myuser -e PW3_GRAFANAPASSWORD=mypass \\\n  -e PW3_WEBNOANONYMOUS=1 -e PW3_WEBNOCOMPONENTLOGS=1 \\\n  -e PW3_WEBUSER=myuser -e PW3_WEBPASSWORD=mypass \\\n  -e PW3_AES_GCM_KEYPHRASE=qwerty \\\n  cybertec/pgwatch3\n</code></pre> <p>For custom installs it's up to the user though. A hint - Docker launcher files can also be inspected to see which config parameters are being touched.</p>"},{"location":"sizing_recommendations/","title":"Sizing recommendations","text":"<ul> <li> <p>Min 1GB of RAM is required for a Docker setup using Postgres to     store metrics.</p> <p>The gatherer alone needs typically less than 50 MB if the metrics store is online. Memory consumption will increase a lot when the metrics store is offline though, as then metrics are cached in RAM in ringbuffer style up to a limit of 10k data points (for all databases) and then memory consumption is dependent on how \"wide\" are the metrics gathered.</p> </li> <li> <p>Storage requirements vary a lot and are hard to predict.</p> <p>10GB of disk space should be enough though for monitoring a single DB with \"exhaustive\" preset for 1 month with Postgres storage. 2 weeks is also the default metrics retention policy for Postgres running in Docker (configurable). Depending on the amount of schema objects - tables, indexes, stored procedures and especially on number of unique SQL-s, it could be also much more. If disk size reduction is wanted for PostgreSQL storage then best would be to use the TimescaleDB extension - it has built-in compression and disk footprint is x5 time less than vanila Postgres, while retaining full SQL support.</p> </li> <li> <p>A low-spec (1 vCPU, 2 GB RAM) cloud machine can easily monitor 100     DBs in \"exhaustive\" settings (i.e. almost all metrics are     monitored in 1-2min intervals) without breaking a sweat (\\&lt;20%     load).</p> </li> <li> <p>A single POstgres node should handle thousands of requests per     second but if this is not enough.</p> </li> <li> <p>When high metrics write latency is problematic (e.g. using a DBaaS     across the atlantic) then increasing the default maximum batching     delay of 250ms usually gives good results.</p> <p>Relevant params: --batching-delay-ms / PW3_BATCHING_MAX_DELAY_MS</p> </li> <li> <p>Note that when monitoring a very large number of databases, it's     possible to \"shard\" / distribute them between many metric     collection instances running on different hosts, via the group     attribute. This requires that some hosts have been assigned a     non-default group identifier, which is just a text field exactly     for this sharding purpose.</p> <p>Relevant params: --group / PW3_GROUP</p> </li> </ul>"},{"location":"technical_details/","title":"Technical details","text":"<p>Here are some technical details that might be interesting for those who are planning to use pgwatch3 for critical monitoring tasks or customize it in some way.</p> <ul> <li> <p>Dynamic management of monitored databases, metrics and their     intervals - no need to restart/redeploy</p> <p>Config DB or YAML / SQL files are scanned every 2 minutes (by default, changeable via --servers-refresh-loop-seconds) and changes are applied dynamically. As common connectivity errors also also handled, there should be no need to restart the gatherer \"for fun\". Please always report issues which require restarting.</p> </li> <li> <p>There are some safety features built-in so that monitoring would not     obstruct actual operation of databases</p> <ul> <li>Up to 2 concurrent queries per monitored database (thus more per     cluster) are allowed</li> <li>Configurable statement timeouts per DB</li> <li>SSL connections support for safe over-the-internet monitoring     (use \"-e PW3_WEBSSL=1 -e PW3_GRAFANASSL=1\" when launching     Docker)</li> <li>Optional authentication for the Web UI and Grafana (by default     freely accessible)</li> </ul> </li> <li> <p>Instance-level metrics caching</p> <p>To further reduce load on multi-DB instances, pgwatch3 can cache the output of metrics that are marked to gather only instance-level data. One such metric is for example \"wal\", and the metric attribute is \"is_instance_level\". Caching will by activated only for continuous <code>DB types &lt;db_types&gt;</code>{.interpreted-text role=\"ref\"}, and to a default limit of up to 30 seconds (changeable via the --instance-level-cache-max-seconds param).</p> </li> </ul>"},{"location":"upgrading/","title":"Upgrading","text":"<p>The pgwatch3 daemon code doesn't need too much maintenance itself (if you're not interested in new features), but the preset metrics, dashboards and the other components that pgwatch3 relies, like Grafana, are under very active development and get updates quite regularly so already purely from the security standpoint it would make sense to stay up to date.</p> <p>We also regularly include new component versions in the Docker images after verifying that they work. If using Docker, you could also choose to build your own images any time some new component versions are released, just increment the version numbers in the Dockerfile.</p>"},{"location":"upgrading/#updating-to-a-newer-docker-version","title":"Updating to a newer Docker version","text":""},{"location":"upgrading/#without-volumes","title":"Without volumes","text":"<p>If pgwatch3 container was started in the simplest way possible without volumes, and if previously gathered metrics are not of great importance, and there are no user modified metric or dashboard changes that should be preserved, then the easiest way to get the latest components would be just to launch new container and import the old monitoring config:</p> <pre><code># let's backup up the monitored hosts\npsql -p5432 -U pgwatch3 -d pgwatch3 -c \"\\copy monitored_db to 'monitored_db.copy'\"\n\n# stop the old container and start a new one ...\ndocker stop ... &amp;&amp; docker run ....\n\n# import the monitored hosts\npsql -p5432 -U pgwatch3 -d pgwatch3 -c \"\\copy monitored_db from 'monitored_db.copy'\"\n</code></pre> <p>If metrics data and other settings like custom dashboards need to be preserved then some more steps are needed, but basically it's about pulling Postgres backups and restoring them into the new container.</p> <p>A tip: to make the restore process easier it would already make sense to mount the host folder with the backups in it on the new container with \"-v \\~/pgwatch3_backups:/pgwatch3_backups:rw,z\" when starting the Docker image. Otherwise one needs to set up SSH or use something like S3 for example. Also note that port 5432 need to be exposed to take backups outside of Docker for Postgres respectively.</p>"},{"location":"upgrading/#with-volumes","title":"With volumes","text":"<p>To make updates a bit easier, the preferred way to launch pgwatch3 should be to use Docker volumes for each individual component - see the <code>Installing using Docker &lt;docker_example_launch&gt;</code>{.interpreted-text role=\"ref\"} chapter for details. Then one can just stop the old container and start a new one, re-using the volumes.</p> <p>With some releases though, updating to newer version might additionally still require manual rollout of Config DB schema migrations scripts, so always check the release notes for hints on that or just go to the \"pgwatch3/sql/migrations\" folder and execute all SQL scripts that have a higher version than the old pgwatch3 container. Error messages like will \"missing columns\" or \"wrong datatype\" will also hint at that, after launching with a new image. FYI - such SQL \"patches\" are generally not provided for metric updates, nor dashboard changes and they need to be updated separately.</p>"},{"location":"upgrading/#updating-without-docker","title":"Updating without Docker","text":"<p>For a custom installation there's quite some freedom in doing updates - as components (Grafana, PostgreSQL) are loosely coupled, they can be updated any time without worrying too much about the other components. Only \"tightly coupled\" components are the pgwatch3 metrics collector, config DB and the optional Web UI - if the pgwatch3 config is kept in the database. If YAML based approach (see details <code>here &lt;yaml_setup&gt;</code>{.interpreted-text role=\"ref\"}) is used, then things are even more simple - the pgwatch3 daemon can be updated any time as YAML schema has default values for everything and there are no other \"tightly coupled\" components like the Web UI.</p>"},{"location":"upgrading/#updating-grafana","title":"Updating Grafana","text":"<p>The update process for Grafana looks pretty much like the installation so take a look at the according <code>chapter &lt;custom_install_grafana&gt;</code>{.interpreted-text role=\"ref\"}. If using Grafana's package repository it should happen automatically along with other system packages. Grafana has a built-in database schema migrator, so updating the binaries and restarting is enough.</p>"},{"location":"upgrading/#updating-grafana-dashboards","title":"Updating Grafana dashboards","text":"<p>There are no update or migration scripts for the built-in Grafana dashboards as it would break possible user applied changes. If you know that there are no user changes, then one can just delete or rename the existing ones in a bulk matter and import the latest JSON definitions. See <code>here &lt;dashboard_maintenance&gt;</code>{.interpreted-text role=\"ref\"} for some more advice on how to manage dashboards.</p>"},{"location":"upgrading/#updating-the-config-metrics-db-version","title":"Updating the config / metrics DB version","text":"<p>Database updates can be quite complex, with many steps, so it makes sense to follow the manufacturer's instructions here.</p> <p>For PostgreSQL one should distinguish between minor version updates and major version upgrades. Minor updates are quite straightforward and problem-free, consisting of running something like:</p> <pre><code>apt update &amp;&amp; apt install postgresql\nsudo systemctl restart postgresql\n</code></pre> <p>For PostgreSQL major version upgrades one should read through the according release notes (e.g. here) and be prepared for the unavoidable downtime.</p>"},{"location":"upgrading/#updating-the-pgwatch3-schema","title":"Updating the pgwatch3 schema","text":"<p>This is the pgwatch3 specific part, with some coupling between the following components - Config DB SQL schema, metrics collector, and the optional Web UI.</p> <p>Here one should check from the CHANGELOG if pgwatch3 schema needs updating. If yes, then manual applying of schema diffs is required before running the new gatherer or Web UI. If no, i.e. no schema changes, all components can be updated independently in random order.</p> <p>Assuming that we initially installed pgwatch3 version v1.6.0, and now the latest version is 1.6.2, based on the release notes and SQL diffs we need to apply the following files:</p> <p>psql -f /etc/pgwatch3/sql/config_store/migrations/v1.6.1-1_patroni_cont_discovery.sql pgwatch3     psql -f /etc/pgwatch3/sql/config_store/migrations/v1.6.2_superuser_metrics.sql pgwatch3</p>"},{"location":"upgrading/#updating-the-metrics-collector","title":"Updating the metrics collector","text":"<p>Compile or install the gatherer from RPM / DEB / tarball packages. See the <code>Custom installation &lt;custom_installation&gt;</code>{.interpreted-text role=\"ref\"} chapter for details.</p> <p>If using a SystemD service file to auto-start the collector then you might want to also check for possible updates on the template there - /etc/pgwatch3/startup-scripts/pgwatch3.service.</p>"},{"location":"upgrading/#updating-the-web-ui","title":"Updating the Web UI","text":"<p>Update the optional Python Web UI if using it to administer monitored DB-s and metric configs. The Web UI was not included in the pre-built packages of older pgwatch3 versions as deploying self-contained Python that runs on all platforms is not overly easy. If Web UI is started directly on the Github sources ([git clone &amp;&amp; cd webpy &amp;&amp; ./web.py]{.title-ref}) then it is actually updated automatically as CherryPy web server monitors the file changes. If there were some breaking schema changes though, it might stop working and needs a restart after applying schema \"diffs\" (see above).</p> <p>If using a SystemD service file to auto-start the Web UI then you might want to also check for possible updates on the template there - /etc/pgwatch/webpy/startup-scripts/pgwatch3-webui.service.</p>"},{"location":"upgrading/#updating-metric-definitions-updating_metrics","title":"Updating metric definitions {#updating_metrics}","text":"<p>In the YAML mode you always get new SQL definitions for the built-in metrics automatically when refreshing the sources via Github or pre-built packages, but with Config DB approach one needs to do it manually. Given that there are no user added metrics, it's simple enough though - just delete all old ones and re-insert everything from the latest metric definition SQL file.</p> <pre><code>pg_dump -t pgwatch3.metric pgwatch3 &gt; old_metric.sql  # a just-in-case backup\npsql  -c \"truncate pgwatch3.metric\" pgwatch3\npsql -f /etc/pgwatch3/sql/config_store/metric_definitions.sql pgwatch3\n</code></pre> <p>If you have added some own custom metrics be sure not to delete or truncate them!</p>"},{"location":"using_managed_services/","title":"Monitoring managed cloud databases","text":"<p>Although all cloud service providers offer some kind of built-in instrumentation and graphs, they're mostly rather conservative in this are not to consume extra server resources and not to overflow and confuse beginners with too much information. So for advanced troubleshooting it might make sense to gather some additional metrics on your own, especially given that you can also easily add custom business metrics to pgwatch3 using plain SQL, for example to track the amount of incoming sales orders. Also with pgwatch3 / Grafana you have more freedom on the visual representation side and access to around 30 prebuilt dashboards and a lot of freedom creating custom alerting rules.</p> <p>The common denominator for all managed cloud services is that they remove / disallow dangerous or potentially dangerous functionalities like file system access and untrusted PL-languages like Python - so you'll lose a small amount of metrics and \"helper functions\" compared to a standard on-site setup described in the <code>previous chapter &lt;preparing_databases&gt;</code>{.interpreted-text role=\"ref\"}. This also means that you will get some errors displayed on some preset dashboards like \"DB overview\" and thus will be better off using a dashboard called \"DB overview Unprivileged\" tailored specially for such a use case.</p> <p>pgwatch3 has been tested to work with the following managed database services:</p>"},{"location":"using_managed_services/#google-cloud-sql-for-postgresql","title":"Google Cloud SQL for PostgreSQL","text":"<ul> <li>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the     Stackdriver     data source</li> <li>\"pg_monitor\" system role available</li> <li>pgwatch3 default preset name: \"gce\"</li> <li>Documentation: https://cloud.google.com/sql/docs/postgres</li> </ul> <p>To get most out pgwatch3 on GCE you need some additional clicks in the GUI / Cloud Console \"Flags\" section to enable some common PostgreSQL monitoring parameters like track_io_timing and track_functions.</p>"},{"location":"using_managed_services/#amazon-rds-for-postgresql","title":"Amazon RDS for PostgreSQL","text":"<ul> <li> <p>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the     CloudWatch     data source</p> </li> <li> <p>\"pg_monitor\" system role available</p> </li> <li> <p>pgwatch3 default preset names: \"rds\", \"aurora\"</p> </li> <li> <p>Documentation:</p> <p>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html</p> </li> </ul> <p>Note that the AWS Aurora PostgreSQL-compatible engine is missing some additional metrics compared to normal RDS.</p>"},{"location":"using_managed_services/#azure-database-for-postgresql","title":"Azure Database for PostgreSQL","text":"<ul> <li>No Python / OS helpers possible. OS metrics can be integrated in     Grafana though using the Azure     Monitor     data source</li> <li>\"pg_monitor\" system role available</li> <li>pgwatch3 default preset name: \"azure\"</li> <li>Documentation: https://docs.microsoft.com/en-us/azure/postgresql/</li> </ul> <p>Surprisingly on Azure some file access functions are whitelisted, thus one can for example use the \"wal_size\" metrics.</p> <p>NB2! By default Azure has pg_stat_statements not fully activated by default, so you need to enable it manually or via the API. Documentation link here.</p>"},{"location":"using_managed_services/#aiven-for-postgresql","title":"Aiven for PostgreSQL","text":"<p>The Aiven developer documentation contains information on how to monitor PostgreSQL instances running on the Aiven platform with pgwatch3.</p>"},{"location":"web_ui/","title":"The Admin Web UI","text":"<p>If using pgwatch3 in the centrally managed Config DB way, for easy configuration management (adding databases to monitoring, adding metrics) there is a small Python Web application bundled making use of the CherryPy Web-framework.</p> <p>For mass configuration changes the Web UI has some buttons to disable / enable all hosts for example, but one could technically also log into the configuration database and change the pgwatch3.monitored_db table directly.</p> <p>Besides managing the metrics gathering configurations, the two other useful features for the Web UI would be the possibility to look at the logs of the single components and to verify that metrics gathering is working on the \"Stat Statements Overview\" page, which will contact the metrics DB (only Postgres is supported) and present some stats summaries.</p> <p>Default port: 8080</p> <p>Sample screenshot of the Web UI:</p> <p></p>"},{"location":"web_ui/#web-ui-security","title":"Web UI security","text":"<p>By default the Web UI is not secured - anyone can view and modify the monitoring configuration. If some security is needed though it can be enabled:</p> <ul> <li> <p>HTTPS</p> <p><code>--ssl, --ssl-cert, --ssl-key, --ssl-certificate-chain</code> or <code>PW3_WEBSSL, PW3_WEBCERT, PW3_WEBKEY, PW3_WEBCERTCHAIN</code></p> </li> <li> <p>Password protection</p> <p><code>--no-anonymous-access, --admin-user, --admin-password</code> or <code>PW3_WEBNOANONYMOUS, PW3_WEBUSER, PW3_WEBPASSWORD</code></p> </li> <li> <p>Hiding some possibly sensitive information</p> <p><code>--no-component-logs, --no-stats-summary</code> or <code>PW3_WEBNOCOMPONENTLOGS, PW3_WEBNOSTATSSUMMARY</code></p> </li> <li> <p>Password encryption for the role used for fetching metrics</p> <p><code>--aes-gcm-keyphrase, --aes-gcm-keyphrase-file</code> or <code>PW3_AES_GCM_KEYPHRASE, PW3_AES_GCM_KEYPHRASE_FILE</code></p> <p>Note that standard LibPQ .pgpass files can also be used so there's no requirement to store any passwords in pgwatch3 config DB. Also note that when enabling password encryption, the same key needs to be presented also for the gatherer.</p> </li> </ul> <p>For security sensitive environments make sure to always deploy password protection together with SSL, as it uses a standard cookie based techniques vulnerable to snooping / MITM attacks.</p>"},{"location":"web_ui/#exposing-the-component-logs","title":"Exposing the component logs","text":"<p>When using the Docker images, internal component logs (Postgres, Grafana, Go daemon, Web UI itself) are exposed via the \"/logs\" endpoint. If this is not wanted set the PW3_WEBNOCOMPONENTLOGS env. variable. Note that if a working \"/logs\" endpoint is desired also in custom setup mode (non-docker) then some actual code changes are needed to specify where logs of all components are situated - see top of the pgwatch3.py file for that.</p>"}]}